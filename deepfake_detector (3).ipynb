{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "JlQz5lFR7Vw-",
      "metadata": {
        "id": "JlQz5lFR7Vw-"
      },
      "outputs": [],
      "source": [
        "import os, sys, time, math, glob, shutil, zipfile, random, hashlib, pickle\n",
        "from pathlib import Path\n",
        "\n",
        "SETUP_FLAG = \"/content/.colab_setup_done\"\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Install or fix environment if imports fail (may restart runtime)\n",
        "# ---------------------------\n",
        "def ensure_env():\n",
        "    try:\n",
        "        import numpy as np, cv2, tensorflow as tf\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Import error or incompatible binaries detected:\", repr(e))\n",
        "        print(\"Installing compatible numpy + opencv + supporting packages. Runtime will restart.\")\n",
        "        os.system(\"pip uninstall -y opencv-python opencv-python-headless >/dev/null 2>&1 || true\")\n",
        "        os.system(\"pip install -q --upgrade pip\")\n",
        "        # Pick numpy compatible with many TF/OpenCV combinations in Colab\n",
        "        os.system(\"pip install -q numpy==1.25.2\")\n",
        "        os.system(\"pip install -q opencv-python-headless==4.7.0.72 scikit-learn matplotlib imageio-ffmpeg tqdm\")\n",
        "        Path(SETUP_FLAG).write_text(\"installed\\n\")\n",
        "        print(\"Restarting runtime to load native extensions (this will disconnect your session)...\")\n",
        "        os.kill(os.getpid(), 9)\n",
        "\n",
        "if not os.path.exists(SETUP_FLAG):\n",
        "    ensure_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "C2e8e73K7l0o",
      "metadata": {
        "id": "C2e8e73K7l0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cfecbc-a589-4488-92fb-1e0ea778b0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mixed precision (mixed_float16) enabled.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Input, Model, callbacks\n",
        "from tensorflow.keras.applications import InceptionResNetV2, EfficientNetB4\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as irv2_preprocess\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Enable mixed precision on GPU (optional)\n",
        "try:\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "        print(\"Mixed precision (mixed_float16) enabled.\")\n",
        "except Exception as e:\n",
        "    print(\"Mixed precision not enabled:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ftbsBk9e7mFt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftbsBk9e7mFt",
        "outputId": "85393eb9-feed-4081-f62a-da7ff78e0d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Found real.zip at /content/drive/MyDrive/real.zip. Extracting .mp4 files to /content/deepfake_dataset/real/real ...\n",
            "Found real.zip at /content/drive/MyDrive/real.zip. Extracting .mp4 files to /content/deepfake_dataset/real/real ...\n",
            "Found fake.zip at /content/drive/MyDrive/fake.zip. Extracting .mp4 files to /content/deepfake_dataset/fake/fakes ...\n",
            "Found fake.zip at /content/drive/MyDrive/fake.zip. Extracting .mp4 files to /content/deepfake_dataset/fake/fakes ...\n",
            "Videos after extraction: real=363, fake=1002\n"
          ]
        }
      ],
      "source": [
        "CACHE_DIR = \"/content/deepfake_cache\"\n",
        "MODEL_PATH = \"/content/deepfake_t4.h5\"\n",
        "DATA_ROOT = \"/content/deepfake_dataset\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "# Try to mount Google Drive (Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "    print(\"Google Drive mounted.\")\n",
        "except Exception as e:\n",
        "    DRIVE_ROOT = None\n",
        "    print(\"Drive mount skipped or unavailable:\", e)\n",
        "\n",
        "def find_and_extract_candidates(names_list, target_subdir):\n",
        "    \"\"\"\n",
        "    Search for any archive name in names_list and extract .mp4 files into DATA_ROOT/target_subdir.\n",
        "    \"\"\"\n",
        "    target_dir = os.path.join(DATA_ROOT, target_subdir)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    search_roots = []\n",
        "    if DRIVE_ROOT and os.path.exists(DRIVE_ROOT):\n",
        "        search_roots.append(DRIVE_ROOT)\n",
        "    search_roots.append(os.getcwd())\n",
        "    found_any = False\n",
        "    for root in search_roots:\n",
        "        for path, dirs, files in os.walk(root):\n",
        "            for fname in names_list:\n",
        "                if fname in files:\n",
        "                    zip_path = os.path.join(path, fname)\n",
        "                    print(f\"Found {fname} at {zip_path}. Extracting .mp4 files to {target_dir} ...\")\n",
        "                    try:\n",
        "                        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "                            for member in z.namelist():\n",
        "                                if member.lower().endswith('.mp4'):\n",
        "                                    member_name = os.path.basename(member)\n",
        "                                    if not member_name:\n",
        "                                        continue\n",
        "                                    dest = os.path.join(target_dir, member_name)\n",
        "                                    with z.open(member) as source, open(dest, \"wb\") as destf:\n",
        "                                        shutil.copyfileobj(source, destf)\n",
        "                        found_any = True\n",
        "                    except zipfile.BadZipFile:\n",
        "                        print(f\"Error: {zip_path} is not a valid zip file.\")\n",
        "                    # stop searching other zip names in this folder once one is handled\n",
        "            # continue walking\n",
        "    if not found_any:\n",
        "        print(f\"Warning: none of {names_list} were found under {search_roots}.\")\n",
        "    return found_any\n",
        "\n",
        "# Prefer real.zip & fake.zip; fall back to r.zip/f.zip\n",
        "find_and_extract_candidates([\"real.zip\",\"r.zip\"], \"real/real\")\n",
        "find_and_extract_candidates([\"fake.zip\",\"f.zip\"], \"fake/fakes\")\n",
        "\n",
        "# Show counts\n",
        "real_count = len(glob.glob(os.path.join(DATA_ROOT,\"real\",\"real\",\"*.mp4\")))\n",
        "fake_count = len(glob.glob(os.path.join(DATA_ROOT,\"fake\",\"fakes\",\"*.mp4\")))\n",
        "print(f\"Videos after extraction: real={real_count}, fake={fake_count}\")\n",
        "if real_count + fake_count == 0:\n",
        "    raise SystemExit(\"No .mp4 files were found in the extracted dataset. Please upload real.zip/fake.zip (or r.zip/f.zip) to Drive or /content.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ejaMIjci7mOm",
      "metadata": {
        "id": "ejaMIjci7mOm"
      },
      "outputs": [],
      "source": [
        "class PatentAlignedDeepfakeDetector:\n",
        "    def __init__(self, max_frames=6, resize_to=(224,224), ascii_grid=(80,40),\n",
        "                 ascii_stats_dim=32, cache_dir=CACHE_DIR):\n",
        "        self.max_frames = max_frames\n",
        "        self.resize_to = resize_to\n",
        "        self.ascii_grid = ascii_grid\n",
        "        self.ascii_stats_dim = ascii_stats_dim\n",
        "        self.ASCII_CHARS = \"@%#*+=-:. \"\n",
        "        self.intensity_bins = np.linspace(0, 255, len(self.ASCII_CHARS))\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    def _cache_key(self, video_path):\n",
        "        return os.path.join(self.cache_dir, hashlib.md5(video_path.encode()).hexdigest()+\".pkl\")\n",
        "\n",
        "    def vectorized_ascii_conversion(self, frame):\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        H, W = gray.shape\n",
        "        gh, gw = self.ascii_grid\n",
        "        if gh <= 0 or gw <= 0:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "        crop_h, crop_w = (H//gh)*gh, (W//gw)*gw\n",
        "        gray = gray[:crop_h,:crop_w] if crop_h>0 and crop_w>0 else gray\n",
        "        if crop_h==0 or crop_w==0:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "        cell_h, cell_w = crop_h//gh, crop_w//gw\n",
        "        try:\n",
        "            cells = gray.reshape(gh,cell_h,gw,cell_w)\n",
        "            means = cells.mean(axis=(1,3))\n",
        "            ascii_idx = np.digitize(means, self.intensity_bins)-1\n",
        "            ascii_idx = np.clip(ascii_idx, 0, len(self.ASCII_CHARS)-1)\n",
        "            ascii_vals = np.array([ord(self.ASCII_CHARS[i]) for i in ascii_idx.flatten()]).reshape(ascii_idx.shape)\n",
        "            ascii_img = np.stack([ascii_vals]*3,axis=-1).astype(np.float32)\n",
        "            return cv2.resize(ascii_img, self.resize_to)\n",
        "        except Exception:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "\n",
        "    def compute_ascii_stats(self, ascii_frames):\n",
        "        ent = []\n",
        "        for f in ascii_frames:\n",
        "            flat = f[:,:,0].flatten()\n",
        "            if len(flat)<3:\n",
        "                ent.append((0.0,0.0))\n",
        "                continue\n",
        "            ent.append((float(np.std(flat)), float(np.mean(flat))))\n",
        "        if len(ent)==0:\n",
        "            packed = np.array([0.0,0.0])\n",
        "        else:\n",
        "            packed = np.mean(ent,axis=0)\n",
        "        reps = int(np.ceil(self.ascii_stats_dim/packed.size))\n",
        "        return np.tile(packed,reps)[:self.ascii_stats_dim].astype(np.float32)\n",
        "\n",
        "    def extract_frames_dual_path(self, video_path):\n",
        "        cache_file = self._cache_key(video_path)\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                return pickle.load(open(cache_file,\"rb\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        pixel, ascii_ = [], []\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        if total > self.max_frames:\n",
        "            idxs = np.linspace(0, total-1, self.max_frames, dtype=int)\n",
        "        elif total > 0:\n",
        "            idxs = range(0, min(total, self.max_frames))\n",
        "        else:\n",
        "            idxs = []\n",
        "        for i in idxs:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, f = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "            f = cv2.resize(f, self.resize_to)[:,:,::-1]\n",
        "            pixel.append(f)\n",
        "            ascii_.append(self.vectorized_ascii_conversion(f))\n",
        "        cap.release()\n",
        "        while len(pixel) < self.max_frames:\n",
        "            if pixel:\n",
        "                pixel.append(pixel[-1].copy())\n",
        "                ascii_.append(ascii_[-1].copy())\n",
        "            else:\n",
        "                pixel.append(np.zeros((*self.resize_to,3), np.uint8))\n",
        "                ascii_.append(np.zeros((*self.resize_to,3), np.uint8))\n",
        "        pixel = np.stack(pixel).astype(np.float32)\n",
        "        ascii_ = np.stack(ascii_).astype(np.float32)\n",
        "        stats = self.compute_ascii_stats(ascii_)\n",
        "        data = (pixel, ascii_, stats)\n",
        "        try:\n",
        "            pickle.dump(data, open(cache_file, \"wb\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return data\n",
        "\n",
        "    def build_patent_aligned_model(self):\n",
        "        T, H, W = self.max_frames, *self.resize_to\n",
        "        pix_in = Input((T,H,W,3), name=\"pixel_input\")\n",
        "        asc_in = Input((T,H,W,3), name=\"ascii_input\")\n",
        "        stats_in = Input((self.ascii_stats_dim,), name=\"ascii_stats_input\")\n",
        "\n",
        "        irv2 = InceptionResNetV2(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(H,W,3))\n",
        "        effb4 = EfficientNetB4(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(H,W,3))\n",
        "        irv2.trainable = False\n",
        "        effb4.trainable = False\n",
        "\n",
        "        pix = layers.TimeDistributed(layers.Lambda(lambda x: irv2(irv2_preprocess(x))))(pix_in)\n",
        "        asc = layers.TimeDistributed(layers.Lambda(lambda x: effb4(eff_preprocess(x))))(asc_in)\n",
        "        stats = layers.RepeatVector(T)(layers.Dense(32, activation=\"relu\")(stats_in))\n",
        "        fused = layers.Concatenate()([pix, asc, stats])\n",
        "\n",
        "        temporal = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(fused)\n",
        "        frame_logits = layers.TimeDistributed(layers.Dense(1, activation=\"sigmoid\"), name=\"frame_logits\")(temporal)\n",
        "        video_pred = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name=\"video_pred\")(frame_logits)\n",
        "\n",
        "        self.model = Model([pix_in, asc_in, stats_in], [video_pred, frame_logits])\n",
        "        return self.model\n",
        "\n",
        "    def _temporal_penalty_loss(self, y_true, frame_logits):\n",
        "        diffs = frame_logits[:,1:,:] - frame_logits[:,:-1,:]\n",
        "        return tf.reduce_mean(tf.abs(diffs))\n",
        "\n",
        "    def compile_model(self, lr=1e-4, temporal_loss_weight=0.1):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy()\n",
        "        def vloss(y_true, y_pred): return bce(y_true, y_pred)\n",
        "        def tloss(y_true, y_pred): return self._temporal_penalty_loss(y_true, y_pred)\n",
        "        self.model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "            loss={\"video_pred\": vloss, \"frame_logits\": tloss},\n",
        "            loss_weights={\"video_pred\": 1.0, \"frame_logits\": temporal_loss_weight},\n",
        "            metrics={\"video_pred\": [tf.keras.metrics.BinaryAccuracy(name=\"acc\")]}\n",
        "        )\n",
        "\n",
        "    def dual_path_generator(self, paths, labels, batch_size=2, augment=False):\n",
        "        while True:\n",
        "            data = list(zip(paths, labels)); random.shuffle(data)\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch = data[i:i+batch_size]\n",
        "                Xp, Xa, Xs, Y = [], [], [], []\n",
        "                for vp, lbl in batch:\n",
        "                    pf, af, st = self.extract_frames_dual_path(vp)\n",
        "                    if augment and random.random() > 0.5:\n",
        "                        pf, af = pf[:, ::-1, :, :], af[:, ::-1, :, :]\n",
        "                    Xp.append(pf.astype(\"float32\"))\n",
        "                    Xa.append(af.astype(\"float32\"))\n",
        "                    Xs.append(st.astype(\"float32\"))\n",
        "                    Y.append(lbl)\n",
        "                Xp = np.stack(Xp)\n",
        "                Xa = np.stack(Xa)\n",
        "                Xs = np.stack(Xs)\n",
        "                Yv = np.array(Y, dtype=np.float32).reshape(-1,1)\n",
        "                Yf = np.zeros((len(Y), self.max_frames, 1), dtype=np.float32)\n",
        "                yield [Xp, Xa, Xs], {\"video_pred\": Yv, \"frame_logits\": Yf}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "U20ZqBgo7mVL",
      "metadata": {
        "id": "U20ZqBgo7mVL"
      },
      "outputs": [],
      "source": [
        "def load_dataset(base_dir=DATA_ROOT, test_size=0.2):\n",
        "    real_dir = os.path.join(base_dir, \"real\", \"real\")\n",
        "    fake_dir = os.path.join(base_dir, \"fake\", \"fakes\")\n",
        "    real = sorted(glob.glob(os.path.join(real_dir, \"*.mp4\")))\n",
        "    fake = sorted(glob.glob(os.path.join(fake_dir, \"*.mp4\")))\n",
        "    print(f\"Found {len(real)} real videos, {len(fake)} fake videos\")\n",
        "    if len(real) + len(fake) == 0:\n",
        "        raise FileNotFoundError(\"No mp4 files found.\")\n",
        "    paths = real + fake\n",
        "    labels = [1]*len(real) + [0]*len(fake)\n",
        "    strat = labels if len(set(labels))>1 else None\n",
        "    return train_test_split(paths, labels, test_size=test_size, stratify=strat, random_state=SEED)\n",
        "\n",
        "def warmup_cache(detector, paths, label=\"train\"):\n",
        "    print(f\"\\n[Warmup] Caching {len(paths)} {label} videos ...\")\n",
        "    for idx, vp in enumerate(paths):\n",
        "        try:\n",
        "            _ = detector.extract_frames_dual_path(vp)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed caching {vp}: {e}\")\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"  Cached {idx+1}/{len(paths)} videos\")\n",
        "    print(f\"[Warmup] Done caching {label}\\n\")\n",
        "\n",
        "def plot_history(history):\n",
        "    hist = history.history\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    if \"loss\" in hist:\n",
        "        plt.plot(hist[\"loss\"], label=\"train_total_loss\")\n",
        "    if \"val_loss\" in hist:\n",
        "        plt.plot(hist[\"val_loss\"], label=\"val_total_loss\")\n",
        "    if \"video_pred_loss\" in hist:\n",
        "        plt.plot(hist[\"video_pred_loss\"], label=\"train_video_loss\")\n",
        "    if \"val_video_pred_loss\" in hist:\n",
        "        plt.plot(hist[\"val_video_pred_loss\"], label=\"val_video_loss\")\n",
        "    plt.title(\"Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    if \"video_pred_acc\" in hist:\n",
        "        plt.plot(hist[\"video_pred_acc\"], label=\"train_video_acc\")\n",
        "    if \"val_video_pred_acc\" in hist:\n",
        "        plt.plot(hist[\"val_video_pred_acc\"], label=\"val_video_acc\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def predict_single(detector, video_path):\n",
        "    pixel_frames, ascii_frames, stats = detector.extract_frames_dual_path(video_path)\n",
        "    Xp = np.expand_dims(pixel_frames.astype(\"float32\"), 0)\n",
        "    Xa = np.expand_dims(ascii_frames.astype(\"float32\"), 0)\n",
        "    Xs = np.expand_dims(stats.astype(\"float32\"), 0)\n",
        "    preds = detector.model.predict([Xp, Xa, Xs], verbose=0)\n",
        "    video_pred, frame_preds = preds\n",
        "    video_score = float(video_pred[0][0])\n",
        "    frame_scores = frame_preds[0,:,0]\n",
        "    return video_score, frame_scores\n",
        "\n",
        "def evaluate_on_validation(detector, val_paths, val_labels, plot_figures=True):\n",
        "    y_true, y_scores = [], []\n",
        "    for vp, lbl in zip(val_paths, val_labels):\n",
        "        try:\n",
        "            score, _ = predict_single(detector, vp)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] predict failed for {vp}: {e}\")\n",
        "            score = 0.5\n",
        "        y_true.append(lbl)\n",
        "        y_scores.append(score)\n",
        "    y_true = np.array(y_true)\n",
        "    y_scores = np.array(y_scores)\n",
        "    y_pred = (y_scores >= 0.5).astype(int)\n",
        "    try:\n",
        "        auc_score = roc_auc_score(y_true, y_scores)\n",
        "    except Exception:\n",
        "        auc_score = float('nan')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cr = classification_report(y_true, y_pred, digits=4)\n",
        "    acc = (y_pred == y_true).mean()\n",
        "    print(\"\\n=== Evaluation on validation set ===\")\n",
        "    print(f\"Samples: {len(y_true)}  Accuracy: {acc:.4f}  AUC: {auc_score:.4f}\")\n",
        "    print(\"\\nClassification report:\\n\", cr)\n",
        "    print(\"\\nConfusion matrix:\\n\", cm)\n",
        "    if plot_figures:\n",
        "        try:\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "            roc_auc_val = auc(fpr, tpr)\n",
        "            plt.figure(figsize=(6,5))\n",
        "            plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_val:.4f}\")\n",
        "            plt.plot([0,1],[0,1],\"--\")\n",
        "            plt.title(\"ROC curve\")\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(\"ROC plot failed:\", e)\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion matrix\")\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(2)\n",
        "        plt.xticks(tick_marks, [\"fake(0)\",\"real(1)\"])\n",
        "        plt.yticks(tick_marks, [\"fake(0)\",\"real(1)\"])\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    return {\"accuracy\": acc, \"auc\": auc_score, \"confusion_matrix\": cm, \"report\": cr, \"y_true\": y_true, \"y_scores\": y_scores}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "hhudPPgB8Kg9",
      "metadata": {
        "id": "hhudPPgB8Kg9"
      },
      "outputs": [],
      "source": [
        "def run_training(base_dir=DATA_ROOT, epochs=8, batch_size=2, resume=True):\n",
        "    train_p, val_p, train_l, val_l = load_dataset(base_dir)\n",
        "    det = PatentAlignedDeepfakeDetector(cache_dir=CACHE_DIR)\n",
        "    det_model = det.build_patent_aligned_model()\n",
        "    det.compile_model()\n",
        "    print(\"Warmup cache (this will process videos and can take time)...\")\n",
        "    warmup_cache(det, train_p, label=\"train\")\n",
        "    warmup_cache(det, val_p, label=\"val\")\n",
        "    # Compute class weights for video_pred output\n",
        "    labels_all = np.array(train_l)\n",
        "    classes = np.unique(labels_all)\n",
        "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=labels_all)\n",
        "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, cw)}\n",
        "    keras_class_weight = {\"video_pred\": class_weight_dict}\n",
        "    print(\"Using class weights:\", keras_class_weight)\n",
        "    train_gen = det.dual_path_generator(train_p, train_l, batch_size=batch_size, augment=True)\n",
        "    val_gen = det.dual_path_generator(val_p, val_l, batch_size=batch_size, augment=False)\n",
        "    steps = max(1, math.ceil(len(train_p) / batch_size))\n",
        "    val_steps = max(1, math.ceil(len(val_p) / batch_size))\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor=\"val_video_pred_acc\", verbose=1),\n",
        "        callbacks.EarlyStopping(monitor=\"val_video_pred_acc\", patience=3, restore_best_weights=True, verbose=1),\n",
        "        callbacks.ReduceLROnPlateau(monitor=\"val_video_pred_acc\", factor=0.5, patience=2, verbose=1)\n",
        "    ]\n",
        "    history = det.model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=cb,\n",
        "        workers=1,\n",
        "        use_multiprocessing=False,\n",
        "        class_weight=keras_class_weight\n",
        "    )\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        det.model.load_weights(MODEL_PATH)\n",
        "    print(\"Training complete. Model saved at:\", MODEL_PATH)\n",
        "    return det, history, (train_p, val_p, train_l, val_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "FamxfseDZ7rx",
      "metadata": {
        "id": "FamxfseDZ7rx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def run_training(base_dir, epochs=10, batch_size=2, resume=True):\n",
        "    \"\"\"\n",
        "    Build detector, warmup cache, create tf.data.Dataset wrappers around\n",
        "    the existing Python generator, and train.\n",
        "    Returns: det, history, (train_p, val_p, train_l, val_l)\n",
        "    \"\"\"\n",
        "    # 1) dataset splits\n",
        "    train_p, val_p, train_l, val_l = load_dataset(base_dir)\n",
        "\n",
        "    # 2) build detector and model\n",
        "    det = PatentAlignedDeepfakeDetector(cache_dir=CACHE_DIR)\n",
        "    model = det.build_patent_aligned_model()\n",
        "    det.compile_model()\n",
        "\n",
        "    # 3) warmup cache (persistent)\n",
        "    warmup_cache(det, train_p, label=\"train\")\n",
        "    warmup_cache(det, val_p,   label=\"val\")\n",
        "\n",
        "    # 4) resume weights if requested\n",
        "    if resume and os.path.exists(MODEL_PATH):\n",
        "        print(f\"\\n[Resume] Loading existing model from {MODEL_PATH}\")\n",
        "        det.model.load_weights(MODEL_PATH)\n",
        "\n",
        "    # 5) create tf.data.Dataset from generator with explicit output_signature\n",
        "    T = det.max_frames\n",
        "    H, W = det.resize_to\n",
        "    ascii_dim = det.ascii_stats_dim\n",
        "\n",
        "    # output_signature mirrors what the generator yields:\n",
        "    # inputs: [Xp, Xa, Xs] -> shapes: (batch, T, H, W, 3), (batch, T, H, W, 3), (batch, ascii_dim)\n",
        "    # outputs: {\"video_pred\": (batch,1), \"frame_logits\": (batch, T, 1)}\n",
        "    input_signature = (\n",
        "        tf.TensorSpec(shape=(None, T, H, W, 3), dtype=tf.float32),   # Xp\n",
        "        tf.TensorSpec(shape=(None, T, H, W, 3), dtype=tf.float32),   # Xa\n",
        "        tf.TensorSpec(shape=(None, ascii_dim), dtype=tf.float32)     # Xs\n",
        "    )\n",
        "    output_signature = {\n",
        "        \"video_pred\": tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
        "        \"frame_logits\": tf.TensorSpec(shape=(None, T, 1), dtype=tf.float32)\n",
        "    }\n",
        "    full_signature = (input_signature, output_signature)\n",
        "\n",
        "    # generator factories (no args)\n",
        "    def train_gen_factory():\n",
        "        return det.dual_path_generator(train_p, train_l, batch_size=batch_size, augment=True)\n",
        "\n",
        "    def val_gen_factory():\n",
        "        return det.dual_path_generator(val_p, val_l, batch_size=batch_size, augment=False)\n",
        "\n",
        "    # Build datasets\n",
        "    train_ds = tf.data.Dataset.from_generator(\n",
        "        train_gen_factory,\n",
        "        output_signature=full_signature\n",
        "    )\n",
        "    val_ds = tf.data.Dataset.from_generator(\n",
        "        val_gen_factory,\n",
        "        output_signature=full_signature\n",
        "    )\n",
        "\n",
        "    # Optional: prefetch to improve throughput\n",
        "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    steps = max(1, len(train_p) // batch_size)\n",
        "    val_steps = max(1, len(val_p) // batch_size)\n",
        "\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True,\n",
        "                                  monitor=\"val_video_pred_acc\", verbose=1),\n",
        "        callbacks.EarlyStopping(monitor=\"val_video_pred_acc\", patience=3,\n",
        "                                restore_best_weights=True),\n",
        "        callbacks.ReduceLROnPlateau(monitor=\"val_video_pred_acc\", factor=0.5,\n",
        "                                    patience=2, verbose=1)\n",
        "    ]\n",
        "\n",
        "    # 6) Fit (no workers/use_multiprocessing)\n",
        "    history = det.model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=cb,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Training complete. Best model saved at: {MODEL_PATH}\")\n",
        "    return det, history, (train_p, val_p, train_l, val_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "XgyIZpG-cKrG",
      "metadata": {
        "id": "XgyIZpG-cKrG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import shutil, time, os\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "class CopyToDriveCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Copy a local checkpoint file to Drive at the end of every epoch (if present).\"\"\"\n",
        "    def __init__(self, src_path, dst_path, copy_on_epoch_end=True):\n",
        "        super().__init__()\n",
        "        self.src = src_path\n",
        "        self.dst = dst_path\n",
        "        self.copy_on_epoch_end = copy_on_epoch_end\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.copy_on_epoch_end and os.path.exists(self.src):\n",
        "            try:\n",
        "                shutil.copy(self.src, self.dst)\n",
        "                now = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
        "                print(f\"[CopyToDrive] Epoch {epoch+1}: copied checkpoint to Drive at {self.dst} ({now})\")\n",
        "            except Exception as e:\n",
        "                print(f\"[CopyToDrive] Failed to copy checkpoint to Drive: {e}\")\n",
        "\n",
        "def run_training(base_dir, epochs=10, batch_size=2, resume=True, local_ckpt=\"/content/deepfake_t4_local.h5\", drive_ckpt=\"/content/drive/MyDrive/deepfake_t4.h5\"):\n",
        "    \"\"\"\n",
        "    Build detector, warmup cache, create tf.data.Dataset wrappers around the existing generator,\n",
        "    train, and checkpoint to local_ckpt while copying it to drive_ckpt each epoch (if drive_ckpt is not None).\n",
        "    Returns: det, history, (train_p, val_p, train_l, val_l)\n",
        "    \"\"\"\n",
        "    # 1) dataset splits\n",
        "    train_p, val_p, train_l, val_l = load_dataset(base_dir)\n",
        "\n",
        "    # 2) build detector and model\n",
        "    det = PatentAlignedDeepfakeDetector(cache_dir=CACHE_DIR)\n",
        "    model = det.build_patent_aligned_model()\n",
        "    det.compile_model()\n",
        "\n",
        "    # 3) warmup cache (persistent)\n",
        "    warmup_cache(det, train_p, label=\"train\")\n",
        "    warmup_cache(det, val_p,   label=\"val\")\n",
        "\n",
        "    # 4) resume from a drive checkpoint if requested (prefer drive_ckpt then fallback local)\n",
        "    if resume:\n",
        "        if drive_ckpt and os.path.exists(drive_ckpt):\n",
        "            try:\n",
        "                det.model.load_weights(drive_ckpt)\n",
        "                print(f\"[Resume] Loaded weights from drive checkpoint: {drive_ckpt}\")\n",
        "            except Exception as e:\n",
        "                print(\"[Resume] Failed to load drive_ckpt:\", e)\n",
        "        elif os.path.exists(local_ckpt):\n",
        "            try:\n",
        "                det.model.load_weights(local_ckpt)\n",
        "                print(f\"[Resume] Loaded weights from local checkpoint: {local_ckpt}\")\n",
        "            except Exception as e:\n",
        "                print(\"[Resume] Failed to load local_ckpt:\", e)\n",
        "\n",
        "    # 5) prepare tf.data.Dataset.from_generator with explicit output_signature\n",
        "    T = det.max_frames\n",
        "    H, W = det.resize_to\n",
        "    ascii_dim = det.ascii_stats_dim\n",
        "\n",
        "    inputs_signature = (\n",
        "        tf.TensorSpec(shape=(None, T, H, W, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, T, H, W, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, ascii_dim), dtype=tf.float32)\n",
        "    )\n",
        "    outputs_signature = {\n",
        "        \"video_pred\": tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
        "        \"frame_logits\": tf.TensorSpec(shape=(None, T, 1), dtype=tf.float32)\n",
        "    }\n",
        "    full_signature = (inputs_signature, outputs_signature)\n",
        "\n",
        "    def train_gen_adapter():\n",
        "        gen = det.dual_path_generator(train_p, train_l, batch_size=batch_size, augment=True)\n",
        "        for batch in gen:\n",
        "            inputs_list, outputs = batch\n",
        "            yield (inputs_list[0], inputs_list[1], inputs_list[2]), outputs\n",
        "\n",
        "    def val_gen_adapter():\n",
        "        gen = det.dual_path_generator(val_p, val_l, batch_size=batch_size, augment=False)\n",
        "        for batch in gen:\n",
        "            inputs_list, outputs = batch\n",
        "            yield (inputs_list[0], inputs_list[1], inputs_list[2]), outputs\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_generator(train_gen_adapter, output_signature=full_signature)\n",
        "    val_ds   = tf.data.Dataset.from_generator(val_gen_adapter, output_signature=full_signature)\n",
        "\n",
        "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    steps = max(1, len(train_p) // batch_size)\n",
        "    val_steps = max(1, len(val_p) // batch_size)\n",
        "\n",
        "    # callbacks: checkpoint locally (fast) and optionally copy to Drive each epoch\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(local_ckpt, save_best_only=True,\n",
        "                                  monitor=\"val_video_pred_acc\", verbose=1, mode=\"max\"),\n",
        "        callbacks.EarlyStopping(monitor=\"val_video_pred_acc\", patience=3,\n",
        "                                restore_best_weights=True, mode=\"max\"),\n",
        "        callbacks.ReduceLROnPlateau(monitor=\"val_video_pred_acc\", factor=0.5,\n",
        "                                    patience=2, verbose=1, mode=\"max\")\n",
        "    ]\n",
        "\n",
        "    # Attach the CopyToDrive callback if drive_ckpt is supplied\n",
        "    if drive_ckpt:\n",
        "        # ensure destination dir exists (safe)\n",
        "        os.makedirs(os.path.dirname(drive_ckpt), exist_ok=True)\n",
        "        cb.append(CopyToDriveCallback(local_ckpt, drive_ckpt))\n",
        "\n",
        "    # 6) Fit\n",
        "    history = det.model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=cb,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Training complete. Best model saved at: {local_ckpt} (and copied to {drive_ckpt} if available)\")\n",
        "    return det, history, (train_p, val_p, train_l, val_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "07p-ZfM6cMkX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07p-ZfM6cMkX",
        "outputId": "af12e329-3495-41d8-80ee-9cc5cd86e06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 363 real videos, 1002 fake videos\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "[Warmup] Caching 1092 train videos ...\n",
            "  Cached 50/1092 videos\n",
            "  Cached 100/1092 videos\n",
            "  Cached 150/1092 videos\n",
            "  Cached 200/1092 videos\n",
            "  Cached 250/1092 videos\n",
            "  Cached 300/1092 videos\n",
            "  Cached 350/1092 videos\n",
            "  Cached 400/1092 videos\n",
            "  Cached 450/1092 videos\n",
            "  Cached 500/1092 videos\n",
            "  Cached 550/1092 videos\n",
            "  Cached 600/1092 videos\n",
            "  Cached 650/1092 videos\n",
            "  Cached 700/1092 videos\n",
            "  Cached 750/1092 videos\n",
            "  Cached 800/1092 videos\n",
            "  Cached 850/1092 videos\n",
            "  Cached 900/1092 videos\n",
            "  Cached 950/1092 videos\n",
            "  Cached 1000/1092 videos\n",
            "  Cached 1050/1092 videos\n",
            "[Warmup] Done caching train\n",
            "\n",
            "\n",
            "[Warmup] Caching 273 val videos ...\n",
            "  Cached 50/273 videos\n",
            "  Cached 100/273 videos\n",
            "  Cached 150/273 videos\n",
            "  Cached 200/273 videos\n",
            "  Cached 250/273 videos\n",
            "[Warmup] Done caching val\n",
            "\n",
            "Epoch 1/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - frame_logits_loss: 0.0224 - loss: 0.6871 - video_pred_acc: 0.6888 - video_pred_loss: 0.6849\n",
            "Epoch 1: val_video_pred_acc improved from -inf to 0.73162, saving model to /content/deepfake_t4_local.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CopyToDrive] Epoch 1: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 06:48:21)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 554ms/step - frame_logits_loss: 0.0224 - loss: 0.6869 - video_pred_acc: 0.6888 - video_pred_loss: 0.6847 - val_frame_logits_loss: 0.0365 - val_loss: 0.5421 - val_video_pred_acc: 0.7316 - val_video_pred_loss: 0.5384 - learning_rate: 1.0000e-04\n",
            "Epoch 2/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - frame_logits_loss: 0.0429 - loss: 0.5047 - video_pred_acc: 0.7757 - video_pred_loss: 0.5004\n",
            "Epoch 2: val_video_pred_acc improved from 0.73162 to 0.82721, saving model to /content/deepfake_t4_local.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CopyToDrive] Epoch 2: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 06:52:42)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 478ms/step - frame_logits_loss: 0.0429 - loss: 0.5047 - video_pred_acc: 0.7757 - video_pred_loss: 0.5005 - val_frame_logits_loss: 0.0593 - val_loss: 0.4770 - val_video_pred_acc: 0.8272 - val_video_pred_loss: 0.4711 - learning_rate: 1.0000e-04\n",
            "Epoch 3/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - frame_logits_loss: 0.0550 - loss: 0.4766 - video_pred_acc: 0.8221 - video_pred_loss: 0.4711\n",
            "Epoch 3: val_video_pred_acc improved from 0.82721 to 0.85662, saving model to /content/deepfake_t4_local.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CopyToDrive] Epoch 3: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 06:56:26)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 410ms/step - frame_logits_loss: 0.0550 - loss: 0.4766 - video_pred_acc: 0.8221 - video_pred_loss: 0.4711 - val_frame_logits_loss: 0.0621 - val_loss: 0.4396 - val_video_pred_acc: 0.8566 - val_video_pred_loss: 0.4334 - learning_rate: 1.0000e-04\n",
            "Epoch 4/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - frame_logits_loss: 0.0576 - loss: 0.4319 - video_pred_acc: 0.8238 - video_pred_loss: 0.4262\n",
            "Epoch 4: val_video_pred_acc did not improve from 0.85662\n",
            "[CopyToDrive] Epoch 4: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:00:47)\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 480ms/step - frame_logits_loss: 0.0576 - loss: 0.4319 - video_pred_acc: 0.8238 - video_pred_loss: 0.4261 - val_frame_logits_loss: 0.0726 - val_loss: 0.5545 - val_video_pred_acc: 0.6912 - val_video_pred_loss: 0.5473 - learning_rate: 1.0000e-04\n",
            "Epoch 5/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - frame_logits_loss: 0.0584 - loss: 0.4077 - video_pred_acc: 0.8435 - video_pred_loss: 0.4019\n",
            "Epoch 5: val_video_pred_acc did not improve from 0.85662\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "[CopyToDrive] Epoch 5: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:04:33)\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 414ms/step - frame_logits_loss: 0.0584 - loss: 0.4077 - video_pred_acc: 0.8435 - video_pred_loss: 0.4019 - val_frame_logits_loss: 0.0691 - val_loss: 0.5051 - val_video_pred_acc: 0.7610 - val_video_pred_loss: 0.4982 - learning_rate: 1.0000e-04\n",
            "Epoch 6/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - frame_logits_loss: 0.0531 - loss: 0.3561 - video_pred_acc: 0.8795 - video_pred_loss: 0.3508\n",
            "Epoch 6: val_video_pred_acc improved from 0.85662 to 0.89338, saving model to /content/deepfake_t4_local.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CopyToDrive] Epoch 6: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:08:56)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 481ms/step - frame_logits_loss: 0.0531 - loss: 0.3561 - video_pred_acc: 0.8795 - video_pred_loss: 0.3508 - val_frame_logits_loss: 0.0462 - val_loss: 0.3447 - val_video_pred_acc: 0.8934 - val_video_pred_loss: 0.3400 - learning_rate: 5.0000e-05\n",
            "Epoch 7/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - frame_logits_loss: 0.0447 - loss: 0.2952 - video_pred_acc: 0.9109 - video_pred_loss: 0.2907\n",
            "Epoch 7: val_video_pred_acc improved from 0.89338 to 0.89706, saving model to /content/deepfake_t4_local.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CopyToDrive] Epoch 7: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:13:18)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 480ms/step - frame_logits_loss: 0.0447 - loss: 0.2952 - video_pred_acc: 0.9109 - video_pred_loss: 0.2907 - val_frame_logits_loss: 0.0416 - val_loss: 0.3350 - val_video_pred_acc: 0.8971 - val_video_pred_loss: 0.3309 - learning_rate: 5.0000e-05\n",
            "Epoch 8/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - frame_logits_loss: 0.0429 - loss: 0.2866 - video_pred_acc: 0.9085 - video_pred_loss: 0.2823\n",
            "Epoch 8: val_video_pred_acc did not improve from 0.89706\n",
            "[CopyToDrive] Epoch 8: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:17:39)\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 480ms/step - frame_logits_loss: 0.0429 - loss: 0.2866 - video_pred_acc: 0.9085 - video_pred_loss: 0.2823 - val_frame_logits_loss: 0.0355 - val_loss: 0.3209 - val_video_pred_acc: 0.8971 - val_video_pred_loss: 0.3174 - learning_rate: 5.0000e-05\n",
            "Epoch 9/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - frame_logits_loss: 0.0367 - loss: 0.2495 - video_pred_acc: 0.9275 - video_pred_loss: 0.2458\n",
            "Epoch 9: val_video_pred_acc did not improve from 0.89706\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "[CopyToDrive] Epoch 9: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:22:02)\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 481ms/step - frame_logits_loss: 0.0367 - loss: 0.2496 - video_pred_acc: 0.9275 - video_pred_loss: 0.2459 - val_frame_logits_loss: 0.0438 - val_loss: 0.3267 - val_video_pred_acc: 0.8934 - val_video_pred_loss: 0.3223 - learning_rate: 5.0000e-05\n",
            "Epoch 10/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - frame_logits_loss: 0.0371 - loss: 0.2403 - video_pred_acc: 0.9223 - video_pred_loss: 0.2366\n",
            "Epoch 10: val_video_pred_acc improved from 0.89706 to 0.91544, saving model to /content/deepfake_t4_local.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CopyToDrive] Epoch 10: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:26:27)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 485ms/step - frame_logits_loss: 0.0371 - loss: 0.2403 - video_pred_acc: 0.9223 - video_pred_loss: 0.2366 - val_frame_logits_loss: 0.0351 - val_loss: 0.2979 - val_video_pred_acc: 0.9154 - val_video_pred_loss: 0.2944 - learning_rate: 2.5000e-05\n",
            "Epoch 11/11\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - frame_logits_loss: 0.0356 - loss: 0.2318 - video_pred_acc: 0.9286 - video_pred_loss: 0.2282\n",
            "Epoch 11: val_video_pred_acc did not improve from 0.91544\n",
            "[CopyToDrive] Epoch 11: copied checkpoint to Drive at /content/drive/MyDrive/deepfake_t4.h5 (2025-09-02 07:30:45)\n",
            "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 474ms/step - frame_logits_loss: 0.0356 - loss: 0.2318 - video_pred_acc: 0.9286 - video_pred_loss: 0.2282 - val_frame_logits_loss: 0.0344 - val_loss: 0.3015 - val_video_pred_acc: 0.9154 - val_video_pred_loss: 0.2980 - learning_rate: 2.5000e-05\n",
            "\n",
            "✅ Training complete. Best model saved at: /content/deepfake_t4_local.h5 (and copied to /content/drive/MyDrive/deepfake_t4.h5 if available)\n",
            "Training finished. Local checkpoint: /content/deepfake_t4_local.h5\n",
            "Drive checkpoint copy: /content/drive/MyDrive/deepfake_t4.h5\n"
          ]
        }
      ],
      "source": [
        "# --- Training invocation (use this cell after defining the updated run_training) ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive (will prompt)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Config\n",
        "DATA_ROOT = \"/content/deepfake_dataset\"\n",
        "EPOCHS = 11\n",
        "BATCH_SIZE = 2\n",
        "RESUME = True\n",
        "\n",
        "# Path strategy (best practice: local checkpoint + persistent Drive copy)\n",
        "LOCAL_MODEL_PATH = \"/content/deepfake_t4_local.h5\"               # fast local checkpoint\n",
        "DRIVE_MODEL_PATH = \"/content/drive/MyDrive/deepfake_t4.h5\"      # persistent copy on Drive\n",
        "DRIVE_CACHE_DIR  = \"/content/drive/MyDrive/deepfake_cache\"      # optional persistent cache\n",
        "\n",
        "# Make Drive cache dir if you want caching persisted\n",
        "os.makedirs(DRIVE_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Set the global CACHE_DIR used by the detector class (so cache persists if desired)\n",
        "CACHE_DIR = DRIVE_CACHE_DIR  # or keep \"/content/deepfake_cache\" if you prefer ephemeral cache\n",
        "\n",
        "# Run training (local checkpoint will be copied to Drive each epoch)\n",
        "detector, history, (train_p, val_p, train_l, val_l) = run_training(\n",
        "    base_dir=DATA_ROOT,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    resume=RESUME,\n",
        "    local_ckpt=LOCAL_MODEL_PATH,\n",
        "    drive_ckpt=DRIVE_MODEL_PATH\n",
        ")\n",
        "\n",
        "print(\"Training finished. Local checkpoint:\", LOCAL_MODEL_PATH)\n",
        "print(\"Drive checkpoint copy:\", DRIVE_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model + weights using the same paths/dirs you used for training\n",
        "# Assumes the following variables already exist in the notebook:\n",
        "#   LOCAL_MODEL_PATH, DRIVE_MODEL_PATH, DRIVE_CACHE_DIR, CACHE_DIR\n",
        "# and detector, history are present after training.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# --- Derive final paths (keeps the same Drive folder used by DRIVE_MODEL_PATH) ---\n",
        "FINAL_LOCAL = \"/content/final.h5\"  # user-requested final filename on Colab local FS\n",
        "FINAL_WEIGHTS_LOCAL = \"/content/final.weights.h5\"\n",
        "\n",
        "# If DRIVE_MODEL_PATH exists in training cell, place final files in same Drive folder\n",
        "drive_folder = os.path.dirname(DRIVE_MODEL_PATH) if 'DRIVE_MODEL_PATH' in globals() else \"/content/drive/MyDrive\"\n",
        "FINAL_DRIVE = os.path.join(drive_folder, \"final.h5\")\n",
        "FINAL_WEIGHTS_DRIVE = os.path.join(drive_folder, \"final.weights.h5\")\n",
        "\n",
        "# Ensure parent directories exist (Drive was mounted earlier in your training cell)\n",
        "os.makedirs(os.path.dirname(FINAL_LOCAL), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(FINAL_DRIVE), exist_ok=True)\n",
        "\n",
        "# 1) Copy local checkpoint (if any) to the Drive checkpoint path used during training\n",
        "try:\n",
        "    if 'LOCAL_MODEL_PATH' in globals() and os.path.exists(LOCAL_MODEL_PATH):\n",
        "        print(f\"Copying local checkpoint {LOCAL_MODEL_PATH} -> {DRIVE_MODEL_PATH}\")\n",
        "        shutil.copy(LOCAL_MODEL_PATH, DRIVE_MODEL_PATH)\n",
        "        print(\"Checkpoint copy complete.\")\n",
        "    else:\n",
        "        print(\"No local checkpoint found at LOCAL_MODEL_PATH; skipping checkpoint copy.\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: could not copy local checkpoint to Drive:\", e)\n",
        "\n",
        "# 2) Save final full model as legacy HDF5 named final.h5 (this will emit the .h5 warning)\n",
        "try:\n",
        "    print(f\"Saving full model locally as: {FINAL_LOCAL}\")\n",
        "    detector.model.save(FINAL_LOCAL)\n",
        "    print(f\"Saving full model to Drive as: {FINAL_DRIVE}\")\n",
        "    detector.model.save(FINAL_DRIVE)\n",
        "    print(\"Full model saved to both local and Drive.\")\n",
        "except Exception as e:\n",
        "    print(\"Error saving full model as .h5 (falling back to .keras):\", e)\n",
        "    # Fallback: save in recommended .keras format\n",
        "    try:\n",
        "        FALLBACK_LOCAL = FINAL_LOCAL.replace(\".h5\", \".keras\")\n",
        "        FALLBACK_DRIVE = FINAL_DRIVE.replace(\".h5\", \".keras\")\n",
        "        print(f\"Saving fallback .keras model to {FALLBACK_LOCAL} and {FALLBACK_DRIVE}\")\n",
        "        detector.model.save(FALLBACK_LOCAL)\n",
        "        detector.model.save(FALLBACK_DRIVE)\n",
        "        print(\"Fallback .keras save successful.\")\n",
        "    except Exception as e2:\n",
        "        print(\"Fallback save also failed:\", e2)\n",
        "\n",
        "# 3) Save weights in HDF5 weights format (filename must end with .weights.h5)\n",
        "try:\n",
        "    print(f\"Saving weights locally as: {FINAL_WEIGHTS_LOCAL}\")\n",
        "    detector.model.save_weights(FINAL_WEIGHTS_LOCAL)\n",
        "    print(f\"Saving weights to Drive as: {FINAL_WEIGHTS_DRIVE}\")\n",
        "    detector.model.save_weights(FINAL_WEIGHTS_DRIVE)\n",
        "    print(\"Weights saved to both local and Drive.\")\n",
        "except Exception as e:\n",
        "    print(\"Error saving weights (.weights.h5):\", e)\n",
        "\n",
        "# 4) Save training history as pickle alongside Drive folder (optional)\n",
        "try:\n",
        "    import pickle\n",
        "    HISTORY_LOCAL = \"/content/history.pkl\"\n",
        "    HISTORY_DRIVE = os.path.join(drive_folder, \"history.pkl\")\n",
        "    with open(HISTORY_LOCAL, \"wb\") as f:\n",
        "        pickle.dump(history.history, f)\n",
        "    with open(HISTORY_DRIVE, \"wb\") as f:\n",
        "        pickle.dump(history.history, f)\n",
        "    print(f\"Saved training history to: {HISTORY_LOCAL} and {HISTORY_DRIVE}\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: could not save training history:\", e)\n",
        "\n",
        "# 5) Summary printout & quick load examples\n",
        "print(\"\\nSummary of saved artifacts:\")\n",
        "print(\" - Local final model:\", FINAL_LOCAL, \"(legacy .h5; Keras may warn)\")\n",
        "print(\" - Drive final model:\", FINAL_DRIVE)\n",
        "print(\" - Local weights:\", FINAL_WEIGHTS_LOCAL)\n",
        "print(\" - Drive weights:\", FINAL_WEIGHTS_DRIVE)\n",
        "print(\" - Drive checkpoint used during training (if copied):\", DRIVE_MODEL_PATH)\n",
        "\n",
        "print(\"\\nExample to load the legacy .h5 full model later:\")\n",
        "print(f\"  from tensorflow.keras.models import load_model\\n  model = load_model('{FINAL_DRIVE}')\")\n",
        "\n",
        "print(\"\\nExample to load weights into a constructed model later:\")\n",
        "print(f\"  model = <your_model_constructor>()\\n  model.load_weights('{FINAL_WEIGHTS_DRIVE}')\")\n",
        "\n",
        "# If you want the model also saved in native Keras (.keras) format, uncomment and run:\n",
        "# detector.model.save(FINAL_LOCAL.replace('.h5', '.keras'))\n",
        "# detector.model.save(FINAL_DRIVE.replace('.h5', '.keras'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT6UwagSWkA1",
        "outputId": "c287921a-010b-4314-d0fc-8578e888dc29"
      },
      "id": "WT6UwagSWkA1",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying local checkpoint /content/deepfake_t4_local.h5 -> /content/drive/MyDrive/deepfake_t4.h5\n",
            "Checkpoint copy complete.\n",
            "Saving full model locally as: /content/final.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving full model to Drive as: /content/drive/MyDrive/final.h5\n",
            "Full model saved to both local and Drive.\n",
            "Saving weights locally as: /content/final.weights.h5\n",
            "Saving weights to Drive as: /content/drive/MyDrive/final.weights.h5\n",
            "Weights saved to both local and Drive.\n",
            "Saved training history to: /content/history.pkl and /content/drive/MyDrive/history.pkl\n",
            "\n",
            "Summary of saved artifacts:\n",
            " - Local final model: /content/final.h5 (legacy .h5; Keras may warn)\n",
            " - Drive final model: /content/drive/MyDrive/final.h5\n",
            " - Local weights: /content/final.weights.h5\n",
            " - Drive weights: /content/drive/MyDrive/final.weights.h5\n",
            " - Drive checkpoint used during training (if copied): /content/drive/MyDrive/deepfake_t4.h5\n",
            "\n",
            "Example to load the legacy .h5 full model later:\n",
            "  from tensorflow.keras.models import load_model\n",
            "  model = load_model('/content/drive/MyDrive/final.h5')\n",
            "\n",
            "Example to load weights into a constructed model later:\n",
            "  model = <your_model_constructor>()\n",
            "  model.load_weights('/content/drive/MyDrive/final.weights.h5')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell A: Load final model (tries Drive final.h5, final.keras, or configured DRIVE_MODEL_PATH)\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Candidate locations (edit if you use different names)\n",
        "candidates = [\n",
        "    \"/content/final.h5\",\n",
        "    \"/content/final.keras\",\n",
        "]\n",
        "if 'DRIVE_MODEL_PATH' in globals():\n",
        "    drive_dir = os.path.dirname(DRIVE_MODEL_PATH)\n",
        "    candidates += [\n",
        "        os.path.join(drive_dir, \"final.h5\"),\n",
        "        os.path.join(drive_dir, \"final.keras\"),\n",
        "        DRIVE_MODEL_PATH\n",
        "    ]\n",
        "\n",
        "loaded_model = None\n",
        "for p in candidates:\n",
        "    if os.path.exists(p):\n",
        "        try:\n",
        "            print(\"Loading model from:\", p)\n",
        "            loaded_model = load_model(p)\n",
        "            print(\"Loaded model from:\", p)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(\"Found model at\", p, \"but failed to load:\", e)\n",
        "\n",
        "# If load failed, keep using detector.model if present\n",
        "if loaded_model is None:\n",
        "    if 'detector' in globals() and hasattr(detector, 'model'):\n",
        "        print(\"Falling back to detector.model already in memory.\")\n",
        "        loaded_model = detector.model\n",
        "    else:\n",
        "        raise RuntimeError(\"No model loaded and no detector.model available. Please ensure a model exists.\")\n",
        "\n",
        "# expose for later cells\n",
        "model = loaded_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuGq0n8XYQwj",
        "outputId": "b8056acf-6a8c-4f51-df18-adf56e2063a5"
      },
      "id": "DuGq0n8XYQwj",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/final.h5\n",
            "Found model at /content/final.h5 but failed to load: The `function` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().\n",
            "Loading model from: /content/drive/MyDrive/final.h5\n",
            "Found model at /content/drive/MyDrive/final.h5 but failed to load: The `function` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().\n",
            "Loading model from: /content/drive/MyDrive/deepfake_t4.h5\n",
            "Found model at /content/drive/MyDrive/deepfake_t4.h5 but failed to load: The `function` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().\n",
            "Falling back to detector.model already in memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook cell: Accuracy/Loss plots + Confusion Matrix + ROC + Metrics\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    classification_report, accuracy_score, balanced_accuracy_score,\n",
        "    precision_recall_fscore_support, roc_curve, auc\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# --- 1) Select model reference ---\n",
        "if 'model' in globals() and hasattr(globals()['model'], 'predict'):\n",
        "    model_ref = globals()['model']\n",
        "elif 'detector' in globals() and hasattr(detector, 'model'):\n",
        "    model_ref = detector.model\n",
        "else:\n",
        "    raise RuntimeError(\"No model found in the notebook. Ensure `detector` or `model` exists.\")\n",
        "\n",
        "# --- 2) Plot training history (accuracy & loss) ---\n",
        "hist = getattr(history, \"history\", history if isinstance(history, dict) else None)\n",
        "if hist is None:\n",
        "    raise RuntimeError(\"`history` not found or invalid. Ensure training produced a `history` object.\")\n",
        "\n",
        "def get_key(d, candidates):\n",
        "    for c in candidates:\n",
        "        if c in d:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "acc_key = get_key(hist, [\"video_pred_acc\", \"video_pred_accuracy\", \"acc\", \"accuracy\"])\n",
        "val_acc_key = get_key(hist, [\"val_video_pred_acc\", \"val_video_pred_accuracy\", \"val_acc\", \"val_accuracy\"])\n",
        "loss_key = get_key(hist, [\"loss\"])\n",
        "val_loss_key = get_key(hist, [\"val_loss\"])\n",
        "\n",
        "epochs = range(1, (len(hist[loss_key]) if loss_key in hist else len(next(iter(hist.values()))) ) + 1)\n",
        "\n",
        "if acc_key and val_acc_key:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(epochs, hist[acc_key], label=\"Train\")\n",
        "    plt.plot(epochs, hist[val_acc_key], label=\"Val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training & Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Accuracy keys not found in history. Available keys:\", list(hist.keys()))\n",
        "\n",
        "if loss_key and val_loss_key:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(epochs, hist[loss_key], label=\"Train\")\n",
        "    plt.plot(epochs, hist[val_loss_key], label=\"Val\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Loss keys not found in history. Available keys:\", list(hist.keys()))\n",
        "\n",
        "# --- 3) Produce predictions on validation set robustly ---\n",
        "if 'val_p' not in globals() or 'val_l' not in globals():\n",
        "    raise RuntimeError(\"Validation data not available in notebook. Ensure `val_p` and `val_l` exist.\")\n",
        "\n",
        "val_paths = val_p\n",
        "val_labels = np.array(val_l)\n",
        "n_val = len(val_paths)\n",
        "batch_for_eval = max(1, globals().get('BATCH_SIZE', 1))  # safe default\n",
        "steps = max(1, math.ceil(n_val / batch_for_eval))\n",
        "\n",
        "preds_batches = []\n",
        "\n",
        "# If detector provides a batch generator, iterate it and call model.predict on each batch\n",
        "if hasattr(detector, \"dual_path_generator\"):\n",
        "    gen = detector.dual_path_generator(val_paths, val_labels.tolist(), batch_size=batch_for_eval, augment=False)\n",
        "    for _ in range(steps):\n",
        "        try:\n",
        "            batch = next(gen)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        # batch can be (inputs, labels) or inputs only\n",
        "        x = batch[0] if isinstance(batch, (list, tuple)) and len(batch) >= 1 else batch\n",
        "        # call predict on the batch inputs (model will accept arrays / lists / dicts)\n",
        "        preds_batches.append(model_ref.predict(x, verbose=0))\n",
        "else:\n",
        "    # fallback: try direct predict on val_paths (works if model accepts file paths or if val_p is an array)\n",
        "    try:\n",
        "        preds_raw_direct = model_ref.predict(val_paths, batch_size=batch_for_eval, verbose=1)\n",
        "        preds_batches.append(preds_raw_direct)\n",
        "    except Exception:\n",
        "        # as final fallback, predict per-sample (slow but robust)\n",
        "        for p in val_paths:\n",
        "            try:\n",
        "                # try to let model accept single sample input: first attempt detector helper if exists\n",
        "                if hasattr(detector, \"prepare_video_for_model\"):\n",
        "                    x_single = detector.prepare_video_for_model(p)\n",
        "                    pred_single = model_ref.predict(np.expand_dims(x_single, axis=0), verbose=0)\n",
        "                else:\n",
        "                    # attempt model.predict on the path as single element list\n",
        "                    pred_single = model_ref.predict([p], verbose=0)\n",
        "                preds_batches.append(pred_single)\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Failed to predict for sample {p}: {e}\")\n",
        "\n",
        "# Aggregate batch-level predictions into preds_raw\n",
        "if len(preds_batches) == 0:\n",
        "    raise RuntimeError(\"No predictions were produced. Check the generator / model input format.\")\n",
        "\n",
        "first_pred = preds_batches[0]\n",
        "if isinstance(first_pred, dict):\n",
        "    # concatenate each dict entry across batches\n",
        "    preds_raw = {}\n",
        "    for k in first_pred.keys():\n",
        "        try:\n",
        "            preds_raw[k] = np.concatenate([batch[k] for batch in preds_batches], axis=0)\n",
        "        except Exception:\n",
        "            preds_raw[k] = np.vstack([np.atleast_2d(batch[k]) for batch in preds_batches])\n",
        "else:\n",
        "    try:\n",
        "        preds_raw = np.concatenate(preds_batches, axis=0)\n",
        "    except Exception:\n",
        "        preds_raw = np.vstack([np.atleast_2d(b) for b in preds_batches])\n",
        "\n",
        "# Extract y_score from preds_raw (support dict outputs like {\"video_pred\": ...})\n",
        "if isinstance(preds_raw, dict):\n",
        "    if \"video_pred\" in preds_raw:\n",
        "        y_score = np.array(preds_raw[\"video_pred\"])\n",
        "    else:\n",
        "        # use first available output\n",
        "        first_key = next(iter(preds_raw.keys()))\n",
        "        y_score = np.array(preds_raw[first_key])\n",
        "else:\n",
        "    y_score = np.array(preds_raw)\n",
        "\n",
        "# Normalize shape and compute y_pred\n",
        "if y_score.ndim == 1:\n",
        "    y_score = y_score.reshape(-1, 1)\n",
        "if y_score.shape[0] != n_val:\n",
        "    # If length mismatch, truncate or pad appropriately (prefer truncation)\n",
        "    y_score = y_score[:n_val]\n",
        "\n",
        "if y_score.shape[1] == 1:\n",
        "    y_pred = (y_score.ravel() >= 0.5).astype(int)\n",
        "else:\n",
        "    y_pred = np.argmax(y_score, axis=1)\n",
        "\n",
        "y_true = val_labels\n",
        "\n",
        "# --- 4) Confusion Matrix ---\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6,6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix (validation)\")\n",
        "plt.show()\n",
        "\n",
        "# --- 5) ROC Curve(s) & AUC ---\n",
        "n_classes = y_score.shape[1] if y_score.ndim == 2 else 1\n",
        "if n_classes == 1:\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_score.ravel())\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.figure(figsize=(6,6))\n",
        "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\")\n",
        "        plt.plot([0,1],[0,1],'k--')\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curve (binary)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(\"Could not compute ROC for binary:\", e)\n",
        "else:\n",
        "    # One-vs-rest ROC for each class\n",
        "    y_true_onehot = to_categorical(y_true, num_classes=n_classes)\n",
        "    plt.figure(figsize=(7,7))\n",
        "    for i in range(n_classes):\n",
        "        try:\n",
        "            fpr, tpr, _ = roc_curve(y_true_onehot[:, i], y_score[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, label=f\"Class {i} (AUC={roc_auc:.3f})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not compute ROC for class {i}: {e}\")\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curves (one-vs-rest)\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# --- 6) Classification report & summary metrics ---\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Balanced accuracy: {bal_acc:.4f}\")\n",
        "print(f\"Weighted Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Attempt AUC summary\n",
        "try:\n",
        "    if n_classes == 1:\n",
        "        auc_score = auc(*roc_curve(y_true, y_score.ravel())[:2])\n",
        "        print(f\"AUC (binary): {auc_score:.4f}\")\n",
        "    else:\n",
        "        auc_macro = roc_auc_score = None\n",
        "        try:\n",
        "            from sklearn.metrics import roc_auc_score as _roc_auc_score\n",
        "            y_true_onehot = to_categorical(y_true, num_classes=n_classes)\n",
        "            auc_macro = _roc_auc_score(y_true_onehot, y_score, average=\"macro\", multi_class=\"ovr\")\n",
        "            auc_weighted = _roc_auc_score(y_true_onehot, y_score, average=\"weighted\", multi_class=\"ovr\")\n",
        "            print(f\"AUC (macro): {auc_macro:.4f}\")\n",
        "            print(f\"AUC (weighted): {auc_weighted:.4f}\")\n",
        "        except Exception:\n",
        "            # ignore if not computable\n",
        "            pass\n",
        "except Exception as e:\n",
        "    print(\"AUC computation failed:\", e)\n",
        "\n",
        "print(\"\\nDone. The variables available for further analysis are: y_score, y_pred, y_true, cm, hist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "Mc9SIHgwYQzi",
        "outputId": "32a32065-2b5e-4753-83d9-62ad856c5549"
      },
      "id": "Mc9SIHgwYQzi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4jFCbMsYQ2Z"
      },
      "id": "e4jFCbMsYQ2Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EADHbzSdYQ5u"
      },
      "id": "EADHbzSdYQ5u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEHbKvTXYRF6"
      },
      "id": "BEHbKvTXYRF6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3l0buJ8KeU6-",
      "metadata": {
        "id": "3l0buJ8KeU6-"
      },
      "outputs": [],
      "source": [
        "# === Post-training / evaluation cell ===\n",
        "# Run this immediately after your training invocation:\n",
        "# detector, history, (train_p, val_p, train_l, val_l) = run_training(...)\n",
        "\n",
        "import os, shutil, json, csv, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Determine model checkpoint to use and Drive paths ---\n",
        "# If you used the CopyToDriveCallback earlier, LOCAL_MODEL_PATH & DRIVE_MODEL_PATH may be set.\n",
        "# Fall back to MODEL_PATH if needed.\n",
        "LOCAL_MODEL_PATH = globals().get(\"LOCAL_MODEL_PATH\", \"/content/deepfake_t4_local.h5\")\n",
        "MODEL_PATH       = globals().get(\"MODEL_PATH\", \"/content/deepfake_t4.h5\")\n",
        "DRIVE_MODEL_PATH = \"/content/drive/MyDrive/deepfake_t4.h5\"\n",
        "DRIVE_CACHE_DIR  = \"/content/drive/MyDrive/deepfake_cache\"\n",
        "\n",
        "# If drive not mounted, mount it\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    # Not in Colab or already mounted — ignore\n",
        "    pass\n",
        "\n",
        "# Choose source checkpoint: prefer LOCAL_MODEL_PATH (fast), then MODEL_PATH\n",
        "if os.path.exists(LOCAL_MODEL_PATH):\n",
        "    src_ckpt = LOCAL_MODEL_PATH\n",
        "elif os.path.exists(MODEL_PATH):\n",
        "    src_ckpt = MODEL_PATH\n",
        "else:\n",
        "    # If detector.model was saved directly to a different path inside callbacks, try to find best .h5 in /content\n",
        "    candidates = [p for p in [\"/content/deepfake_t4.h5\",\"/content/model.h5\",\"/content/best_model.h5\"] if os.path.exists(p)]\n",
        "    src_ckpt = candidates[0] if candidates else None\n",
        "\n",
        "print(\"Using checkpoint:\", src_ckpt)\n",
        "\n",
        "# Ensure Drive destination directory exists\n",
        "os.makedirs(os.path.dirname(DRIVE_MODEL_PATH), exist_ok=True)\n",
        "os.makedirs(DRIVE_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Copy to Drive (if available)\n",
        "if src_ckpt and os.path.exists(src_ckpt):\n",
        "    try:\n",
        "        shutil.copy(src_ckpt, DRIVE_MODEL_PATH)\n",
        "        print(f\"Copied checkpoint to Drive: {DRIVE_MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: failed to copy checkpoint to Drive:\", e)\n",
        "else:\n",
        "    print(\"Warning: no checkpoint file found to copy to Drive.\")\n",
        "\n",
        "# --- 2. Load checkpoint weights into detector.model (if not already loaded) ---\n",
        "if src_ckpt and os.path.exists(src_ckpt):\n",
        "    try:\n",
        "        detector.model.load_weights(src_ckpt)\n",
        "        print(\"Loaded weights from:\", src_ckpt)\n",
        "    except Exception as e:\n",
        "        print(\"Could not load weights from checkpoint:\", e)\n",
        "\n",
        "# --- 3. Plot training history (loss & video accuracy if available) ---\n",
        "try:\n",
        "    h = history.history\n",
        "    plt.figure(figsize=(10,4))\n",
        "    # Loss plot\n",
        "    plt.subplot(1,2,1)\n",
        "    # possible keys: 'loss', 'val_loss' OR 'video_pred_loss', 'val_video_pred_loss' etc.\n",
        "    if \"loss\" in h:\n",
        "        plt.plot(h.get(\"loss\",[]), label=\"train loss\")\n",
        "        plt.plot(h.get(\"val_loss\",[]), label=\"val loss\")\n",
        "    else:\n",
        "        # try top-level video_pred loss keys\n",
        "        vloss = h.get(\"video_pred_loss\", h.get(\"loss\", []))\n",
        "        vvall  = h.get(\"val_video_pred_loss\", h.get(\"val_loss\", []))\n",
        "        if len(vloss)>0:\n",
        "            plt.plot(vloss, label=\"train video loss\")\n",
        "            plt.plot(vvall, label=\"val video loss\")\n",
        "    plt.legend(); plt.title(\"Loss\")\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1,2,2)\n",
        "    # metric name may be 'video_pred_acc' or 'video_pred_binary_accuracy' or similar\n",
        "    acc_keys = [k for k in h.keys() if \"video_pred\" in k and \"acc\" in k or k.endswith(\"acc\")]\n",
        "    if acc_keys:\n",
        "        for k in acc_keys:\n",
        "            plt.plot(h[k], label=k)\n",
        "        # val variants\n",
        "        val_keys = [k for k in h.keys() if k.startswith(\"val_\") and (\"video_pred\" in k or k.endswith(\"acc\"))]\n",
        "        for k in val_keys:\n",
        "            plt.plot(h[k], label=k)\n",
        "    else:\n",
        "        # fallback: try 'acc' and 'val_acc'\n",
        "        if \"video_pred_acc\" in h or \"val_video_pred_acc\" in h:\n",
        "            plt.plot(h.get(\"video_pred_acc\", []), label=\"train acc\")\n",
        "            plt.plot(h.get(\"val_video_pred_acc\", []), label=\"val acc\")\n",
        "    plt.legend(); plt.title(\"Accuracy / Metrics\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Could not plot history:\", e)\n",
        "\n",
        "# --- 4. Run predictions on the validation set and compute metrics ---\n",
        "print(\"\\nRunning predictions on validation set (this may take a while)...\")\n",
        "y_true = []\n",
        "y_scores = []\n",
        "video_paths = []\n",
        "\n",
        "for vp, true_lbl in zip(val_p, val_l):\n",
        "    try:\n",
        "        score, _ = predict_video(detector, vp)   # uses detector.model weights loaded above\n",
        "        y_true.append(int(true_lbl))\n",
        "        y_scores.append(float(score))\n",
        "        video_paths.append(vp)\n",
        "    except Exception as e:\n",
        "        # If a particular video fails, print and skip\n",
        "        print(\"Skipping\", vp, \"-> error:\", e)\n",
        "\n",
        "print(f\"Predicted {len(y_scores)} / {len(val_p)} validation videos.\")\n",
        "\n",
        "# --- 5. Compute metrics and confusion matrix ---\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "preds = [1 if s >= 0.5 else 0 for s in y_scores]\n",
        "acc = accuracy_score(y_true, preds) if len(y_true)>0 else float(\"nan\")\n",
        "auc = roc_auc_score(y_true, y_scores) if len(set(y_true))>1 else float(\"nan\")\n",
        "cm = confusion_matrix(y_true, preds).tolist() if len(y_true)>0 else None\n",
        "\n",
        "print(\"\\nValidation metrics:\")\n",
        "print(\" Accuracy:\", acc)\n",
        "print(\" AUC:\", auc)\n",
        "print(\" Confusion matrix:\", cm)\n",
        "\n",
        "# --- 6. Save evaluation outputs to Drive (JSON + CSV) ---\n",
        "eval_summary = {\"accuracy\": float(acc) if acc is not None else None,\n",
        "                \"auc\": float(auc) if auc is not None else None,\n",
        "                \"confusion_matrix\": cm}\n",
        "\n",
        "EVAL_JSON = \"/content/drive/MyDrive/deepfake_eval_summary.json\"\n",
        "EVAL_CSV  = \"/content/drive/MyDrive/deepfake_eval_per_video.csv\"\n",
        "os.makedirs(os.path.dirname(EVAL_JSON), exist_ok=True)\n",
        "\n",
        "with open(EVAL_JSON, \"w\") as f:\n",
        "    json.dump(eval_summary, f)\n",
        "print(\"Saved eval summary JSON to\", EVAL_JSON)\n",
        "\n",
        "with open(EVAL_CSV, \"w\", newline=\"\") as cf:\n",
        "    writer = csv.writer(cf)\n",
        "    writer.writerow([\"video_path\", \"true_label\", \"score\", \"pred_label\"])\n",
        "    for vp, t, s, p in zip(video_paths, y_true, y_scores, preds):\n",
        "        writer.writerow([vp, int(t), float(s), int(p)])\n",
        "print(\"Saved per-video CSV to\", EVAL_CSV)\n",
        "\n",
        "# --- 7. Show a few example predictions (first 6) ---\n",
        "print(\"\\nSample predictions:\")\n",
        "for vp, t, s in list(zip(video_paths, y_true, y_scores))[:6]:\n",
        "    print(os.path.basename(vp), \"| true:\", t, \"| score(real=1):\", f\"{s:.4f}\", \"| pred:\", int(s>=0.5))\n",
        "\n",
        "# --- 8. (Optional) copy final checkpoint again to a timestamped path on Drive ---\n",
        "try:\n",
        "    if src_ckpt and os.path.exists(src_ckpt):\n",
        "        stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        dst = f\"/content/drive/MyDrive/deepfake_t4_{stamp}.h5\"\n",
        "        shutil.copy(src_ckpt, dst)\n",
        "        print(\"Also copied checkpoint to\", dst)\n",
        "except Exception as e:\n",
        "    print(\"Could not copy timestamped checkpoint:\", e)\n",
        "\n",
        "print(\"\\nPost-training evaluation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HJi3NK32ao05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJi3NK32ao05",
        "outputId": "333424f1-998c-4904-daed-16cba7b88ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training run\n",
            "DATA_ROOT: /content/deepfake_dataset EPOCHS: 10 BATCH_SIZE: 2 RESUME: True\n",
            "Found 363 real videos, 1002 fake videos\n",
            "\n",
            "[Warmup] Caching 1092 train videos ...\n",
            "  Cached 50/1092 videos\n",
            "  Cached 100/1092 videos\n",
            "  Cached 150/1092 videos\n",
            "  Cached 200/1092 videos\n",
            "  Cached 250/1092 videos\n",
            "  Cached 300/1092 videos\n",
            "  Cached 350/1092 videos\n",
            "  Cached 400/1092 videos\n",
            "  Cached 450/1092 videos\n",
            "  Cached 500/1092 videos\n",
            "  Cached 550/1092 videos\n",
            "  Cached 600/1092 videos\n",
            "  Cached 650/1092 videos\n",
            "  Cached 700/1092 videos\n",
            "  Cached 750/1092 videos\n",
            "  Cached 800/1092 videos\n",
            "  Cached 850/1092 videos\n",
            "  Cached 900/1092 videos\n",
            "  Cached 950/1092 videos\n",
            "  Cached 1000/1092 videos\n",
            "  Cached 1050/1092 videos\n",
            "[Warmup] Done caching train\n",
            "\n",
            "\n",
            "[Warmup] Caching 273 val videos ...\n",
            "  Cached 50/273 videos\n",
            "  Cached 100/273 videos\n",
            "  Cached 150/273 videos\n",
            "  Cached 200/273 videos\n",
            "  Cached 250/273 videos\n",
            "[Warmup] Done caching val\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was ((tf.float32, tf.float32, tf.float32), {'video_pred': tf.float32, 'frame_logits': tf.float32}), but the yielded element was ([array([[[[[ 32.,  39.,  51.],\n          [ 31.,  38.,  50.],\n          [ 32.,  36.,  49.],\n          ...,\n          [ 43.,  38.,  37.],\n          [ 31.,  29.,  32.],\n          [ 34.,  33.,  36.]],\n\n         [[ 32.,  39.,  51.],\n          [ 31.,  38.,  50.],\n          [ 32.,  36.,  49.],\n          ...,\n          [ 41.,  36.,  35.],\n          [ 29.,  28.,  31.],\n          [ 43.,  42.,  45.]],\n\n         [[ 34.,  39.,  49.],\n          [ 32.,  37.,  47.],\n          [ 32.,  37.,  47.],\n          ...,\n          [ 32.,  30.,  32.],\n          [ 31.,  29.,  33.],\n          [ 39.,  38.,  42.]],\n\n         ...,\n\n         [[107., 110., 121.],\n          [107., 110., 121.],\n          [107., 110., 121.],\n          ...,\n          [  3.,  21.,  20.],\n          [  3.,  28.,  25.],\n          [  3.,  27.,  24.]],\n\n         [[107., 110., 121.],\n          [107., 110., 121.],\n          [107., 110., 121.],\n          ...,\n          [  0.,  14.,  14.],\n          [  0.,  26.,  26.],\n          [  2.,  29.,  26.]],\n\n         [[107., 110., 121.],\n          [107., 110., 121.],\n          [107., 110., 121.],\n          ...,\n          [  1.,  22.,  20.],\n          [  4.,  32.,  26.],\n          [  6.,  34.,  30.]]],\n\n\n        [[[ 34.,  38.,  51.],\n          [ 32.,  37.,  47.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 83.,  80.,  86.],\n          [ 90.,  89.,  92.],\n          [ 92.,  91.,  94.]],\n\n         [[ 38.,  42.,  55.],\n          [ 32.,  37.,  47.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 49.,  46.,  52.],\n          [ 51.,  51.,  51.],\n          [ 53.,  53.,  53.]],\n\n         [[ 37.,  41.,  54.],\n          [ 32.,  37.,  47.],\n          [ 31.,  36.,  46.],\n          ...,\n          [ 36.,  33.,  39.],\n          [ 43.,  42.,  45.],\n          [ 41.,  40.,  43.]],\n\n         ...,\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [  2.,  32.,  25.],\n          [  1.,  27.,  21.],\n          [  2.,  27.,  16.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [  6.,  37.,  27.],\n          [  8.,  37.,  25.],\n          [  4.,  30.,  17.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [ 12.,  43.,  30.],\n          [ 24.,  54.,  33.],\n          [ 11.,  39.,  21.]]],\n\n\n        [[[ 33.,  37.,  50.],\n          [ 31.,  35.,  48.],\n          [ 27.,  34.,  46.],\n          ...,\n          [ 37.,  35.,  36.],\n          [ 29.,  28.,  31.],\n          [ 32.,  29.,  35.]],\n\n         [[ 33.,  37.,  50.],\n          [ 31.,  35.,  48.],\n          [ 27.,  34.,  46.],\n          ...,\n          [ 39.,  37.,  38.],\n          [ 27.,  26.,  28.],\n          [ 44.,  41.,  47.]],\n\n         [[ 32.,  37.,  47.],\n          [ 29.,  34.,  44.],\n          [ 31.,  36.,  46.],\n          ...,\n          [ 30.,  28.,  33.],\n          [ 29.,  30.,  34.],\n          [ 37.,  37.,  42.]],\n\n         ...,\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [  4.,  18.,  21.],\n          [  3.,  28.,  26.],\n          [  4.,  28.,  25.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [105., 107., 121.],\n          ...,\n          [  0.,  12.,  13.],\n          [  1.,  26.,  23.],\n          [  3.,  29.,  28.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [105., 107., 121.],\n          ...,\n          [  3.,  19.,  19.],\n          [  6.,  31.,  28.],\n          [  9.,  36.,  32.]]],\n\n\n        [[[ 32.,  36.,  49.],\n          [ 31.,  35.,  48.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 38.,  33.,  35.],\n          [ 27.,  27.,  27.],\n          [ 31.,  29.,  37.]],\n\n         [[ 32.,  36.,  49.],\n          [ 31.,  35.,  48.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 41.,  36.,  38.],\n          [ 28.,  27.,  32.],\n          [ 41.,  39.,  47.]],\n\n         [[ 32.,  37.,  47.],\n          [ 29.,  34.,  44.],\n          [ 31.,  36.,  46.],\n          ...,\n          [ 29.,  28.,  33.],\n          [ 29.,  29.,  34.],\n          [ 38.,  37.,  42.]],\n\n         ...,\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  4.,  19.,  19.],\n          [  3.,  28.,  26.],\n          [  1.,  25.,  25.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  0.,  12.,  11.],\n          [  1.,  25.,  25.],\n          [  4.,  28.,  25.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  3.,  19.,  17.],\n          [  2.,  32.,  27.],\n          [  8.,  37.,  30.]]],\n\n\n        [[[ 30.,  37.,  49.],\n          [ 29.,  36.,  48.],\n          [ 29.,  33.,  46.],\n          ...,\n          [ 39.,  37.,  38.],\n          [ 30.,  29.,  32.],\n          [ 29.,  30.,  35.]],\n\n         [[ 30.,  37.,  49.],\n          [ 29.,  36.,  48.],\n          [ 31.,  35.,  48.],\n          ...,\n          [ 39.,  37.,  38.],\n          [ 25.,  24.,  29.],\n          [ 41.,  42.,  47.]],\n\n         [[ 32.,  37.,  47.],\n          [ 28.,  33.,  43.],\n          [ 31.,  35.,  48.],\n          ...,\n          [ 29.,  29.,  29.],\n          [ 30.,  28.,  31.],\n          [ 38.,  37.,  42.]],\n\n         ...,\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  2.,  17.,  17.],\n          [  2.,  27.,  24.],\n          [  1.,  28.,  25.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  0.,  13.,  14.],\n          [  0.,  25.,  22.],\n          [  5.,  30.,  27.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  2.,  20.,  19.],\n          [  3.,  32.,  26.],\n          [  9.,  36.,  32.]]],\n\n\n        [[[ 33.,  37.,  52.],\n          [ 32.,  36.,  49.],\n          [ 29.,  36.,  48.],\n          ...,\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.]],\n\n         [[ 33.,  37.,  52.],\n          [ 31.,  35.,  48.],\n          [ 30.,  37.,  49.],\n          ...,\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.]],\n\n         [[ 33.,  38.,  48.],\n          [ 31.,  36.,  46.],\n          [ 32.,  37.,  47.],\n          ...,\n          [  0.,   0.,   4.],\n          [  1.,   0.,   5.],\n          [  1.,   0.,   5.]],\n\n         ...,\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [152., 111., 122.],\n          [164., 123., 137.],\n          [152., 115., 130.]],\n\n         [[105., 107., 121.],\n          [105., 107., 121.],\n          [105., 107., 121.],\n          ...,\n          [153., 114., 125.],\n          [159., 122., 138.],\n          [140., 105., 120.]],\n\n         [[105., 107., 121.],\n          [105., 107., 121.],\n          [105., 107., 121.],\n          ...,\n          [148., 112., 122.],\n          [149., 113., 123.],\n          [132.,  98., 110.]]]],\n\n\n\n       [[[[ 30.,  25.,  22.],\n          [ 25.,  20.,  17.],\n          [ 35.,  30.,  27.],\n          ...,\n          [ 14.,   9.,  11.],\n          [ 27.,  22.,  24.],\n          [ 55.,  50.,  52.]],\n\n         [[111., 106., 103.],\n          [166., 162., 159.],\n          [153., 148., 145.],\n          ...,\n          [111., 109., 110.],\n          [119., 117., 118.],\n          [115., 113., 114.]],\n\n         [[ 99.,  94.,  91.],\n          [ 57.,  52.,  49.],\n          [ 49.,  44.,  41.],\n          ...,\n          [ 62.,  62.,  59.],\n          [ 47.,  47.,  44.],\n          [ 29.,  29.,  26.]],\n\n         ...,\n\n         [[199., 206., 199.],\n          [201. [Op:__inference_multi_step_on_iterator_414719]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3660707458.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Run training (returns detector, history, (train_p, val_p, train_l, val_l))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m detector, history, (train_p, val_p, train_l, val_l) = run_training(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-51085323.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(base_dir, epochs, batch_size, resume)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# 6) Fit (no workers/use_multiprocessing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     history = det.model.fit(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was ((tf.float32, tf.float32, tf.float32), {'video_pred': tf.float32, 'frame_logits': tf.float32}), but the yielded element was ([array([[[[[ 32.,  39.,  51.],\n          [ 31.,  38.,  50.],\n          [ 32.,  36.,  49.],\n          ...,\n          [ 43.,  38.,  37.],\n          [ 31.,  29.,  32.],\n          [ 34.,  33.,  36.]],\n\n         [[ 32.,  39.,  51.],\n          [ 31.,  38.,  50.],\n          [ 32.,  36.,  49.],\n          ...,\n          [ 41.,  36.,  35.],\n          [ 29.,  28.,  31.],\n          [ 43.,  42.,  45.]],\n\n         [[ 34.,  39.,  49.],\n          [ 32.,  37.,  47.],\n          [ 32.,  37.,  47.],\n          ...,\n          [ 32.,  30.,  32.],\n          [ 31.,  29.,  33.],\n          [ 39.,  38.,  42.]],\n\n         ...,\n\n         [[107., 110., 121.],\n          [107., 110., 121.],\n          [107., 110., 121.],\n          ...,\n          [  3.,  21.,  20.],\n          [  3.,  28.,  25.],\n          [  3.,  27.,  24.]],\n\n         [[107., 110., 121.],\n          [107., 110., 121.],\n          [107., 110., 121.],\n          ...,\n          [  0.,  14.,  14.],\n          [  0.,  26.,  26.],\n          [  2.,  29.,  26.]],\n\n         [[107., 110., 121.],\n          [107., 110., 121.],\n          [107., 110., 121.],\n          ...,\n          [  1.,  22.,  20.],\n          [  4.,  32.,  26.],\n          [  6.,  34.,  30.]]],\n\n\n        [[[ 34.,  38.,  51.],\n          [ 32.,  37.,  47.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 83.,  80.,  86.],\n          [ 90.,  89.,  92.],\n          [ 92.,  91.,  94.]],\n\n         [[ 38.,  42.,  55.],\n          [ 32.,  37.,  47.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 49.,  46.,  52.],\n          [ 51.,  51.,  51.],\n          [ 53.,  53.,  53.]],\n\n         [[ 37.,  41.,  54.],\n          [ 32.,  37.,  47.],\n          [ 31.,  36.,  46.],\n          ...,\n          [ 36.,  33.,  39.],\n          [ 43.,  42.,  45.],\n          [ 41.,  40.,  43.]],\n\n         ...,\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [  2.,  32.,  25.],\n          [  1.,  27.,  21.],\n          [  2.,  27.,  16.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [  6.,  37.,  27.],\n          [  8.,  37.,  25.],\n          [  4.,  30.,  17.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [ 12.,  43.,  30.],\n          [ 24.,  54.,  33.],\n          [ 11.,  39.,  21.]]],\n\n\n        [[[ 33.,  37.,  50.],\n          [ 31.,  35.,  48.],\n          [ 27.,  34.,  46.],\n          ...,\n          [ 37.,  35.,  36.],\n          [ 29.,  28.,  31.],\n          [ 32.,  29.,  35.]],\n\n         [[ 33.,  37.,  50.],\n          [ 31.,  35.,  48.],\n          [ 27.,  34.,  46.],\n          ...,\n          [ 39.,  37.,  38.],\n          [ 27.,  26.,  28.],\n          [ 44.,  41.,  47.]],\n\n         [[ 32.,  37.,  47.],\n          [ 29.,  34.,  44.],\n          [ 31.,  36.,  46.],\n          ...,\n          [ 30.,  28.,  33.],\n          [ 29.,  30.,  34.],\n          [ 37.,  37.,  42.]],\n\n         ...,\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [106., 109., 120.],\n          ...,\n          [  4.,  18.,  21.],\n          [  3.,  28.,  26.],\n          [  4.,  28.,  25.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [105., 107., 121.],\n          ...,\n          [  0.,  12.,  13.],\n          [  1.,  26.,  23.],\n          [  3.,  29.,  28.]],\n\n         [[106., 109., 120.],\n          [106., 109., 120.],\n          [105., 107., 121.],\n          ...,\n          [  3.,  19.,  19.],\n          [  6.,  31.,  28.],\n          [  9.,  36.,  32.]]],\n\n\n        [[[ 32.,  36.,  49.],\n          [ 31.,  35.,  48.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 38.,  33.,  35.],\n          [ 27.,  27.,  27.],\n          [ 31.,  29.,  37.]],\n\n         [[ 32.,  36.,  49.],\n          [ 31.,  35.,  48.],\n          [ 29.,  36.,  48.],\n          ...,\n          [ 41.,  36.,  38.],\n          [ 28.,  27.,  32.],\n          [ 41.,  39.,  47.]],\n\n         [[ 32.,  37.,  47.],\n          [ 29.,  34.,  44.],\n          [ 31.,  36.,  46.],\n          ...,\n          [ 29.,  28.,  33.],\n          [ 29.,  29.,  34.],\n          [ 38.,  37.,  42.]],\n\n         ...,\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  4.,  19.,  19.],\n          [  3.,  28.,  26.],\n          [  1.,  25.,  25.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  0.,  12.,  11.],\n          [  1.,  25.,  25.],\n          [  4.,  28.,  25.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  3.,  19.,  17.],\n          [  2.,  32.,  27.],\n          [  8.,  37.,  30.]]],\n\n\n        [[[ 30.,  37.,  49.],\n          [ 29.,  36.,  48.],\n          [ 29.,  33.,  46.],\n          ...,\n          [ 39.,  37.,  38.],\n          [ 30.,  29.,  32.],\n          [ 29.,  30.,  35.]],\n\n         [[ 30.,  37.,  49.],\n          [ 29.,  36.,  48.],\n          [ 31.,  35.,  48.],\n          ...,\n          [ 39.,  37.,  38.],\n          [ 25.,  24.,  29.],\n          [ 41.,  42.,  47.]],\n\n         [[ 32.,  37.,  47.],\n          [ 28.,  33.,  43.],\n          [ 31.,  35.,  48.],\n          ...,\n          [ 29.,  29.,  29.],\n          [ 30.,  28.,  31.],\n          [ 38.,  37.,  42.]],\n\n         ...,\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  2.,  17.,  17.],\n          [  2.,  27.,  24.],\n          [  1.,  28.,  25.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  0.,  13.,  14.],\n          [  0.,  25.,  22.],\n          [  5.,  30.,  27.]],\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [  2.,  20.,  19.],\n          [  3.,  32.,  26.],\n          [  9.,  36.,  32.]]],\n\n\n        [[[ 33.,  37.,  52.],\n          [ 32.,  36.,  49.],\n          [ 29.,  36.,  48.],\n          ...,\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.]],\n\n         [[ 33.,  37.,  52.],\n          [ 31.,  35.,  48.],\n          [ 30.,  37.,  49.],\n          ...,\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.],\n          [  1.,   0.,   7.]],\n\n         [[ 33.,  38.,  48.],\n          [ 31.,  36.,  46.],\n          [ 32.,  37.,  47.],\n          ...,\n          [  0.,   0.,   4.],\n          [  1.,   0.,   5.],\n          [  1.,   0.,   5.]],\n\n         ...,\n\n         [[105., 108., 119.],\n          [105., 108., 119.],\n          [105., 108., 119.],\n          ...,\n          [152., 111., 122.],\n          [164., 123., 137.],\n          [152., 115., 130.]],\n\n         [[105., 107., 121.],\n          [105., 107., 121.],\n          [105., 107., 121.],\n          ...,\n          [153., 114., 125.],\n          [159., 122., 138.],\n          [140., 105., 120.]],\n\n         [[105., 107., 121.],\n          [105., 107., 121.],\n          [105., 107., 121.],\n          ...,\n          [148., 112., 122.],\n          [149., 113., 123.],\n          [132.,  98., 110.]]]],\n\n\n\n       [[[[ 30.,  25.,  22.],\n          [ 25.,  20.,  17.],\n          [ 35.,  30.,  27.],\n          ...,\n          [ 14.,   9.,  11.],\n          [ 27.,  22.,  24.],\n          [ 55.,  50.,  52.]],\n\n         [[111., 106., 103.],\n          [166., 162., 159.],\n          [153., 148., 145.],\n          ...,\n          [111., 109., 110.],\n          [119., 117., 118.],\n          [115., 113., 114.]],\n\n         [[ 99.,  94.,  91.],\n          [ 57.,  52.,  49.],\n          [ 49.,  44.,  41.],\n          ...,\n          [ 62.,  62.,  59.],\n          [ 47.,  47.,  44.],\n          [ 29.,  29.,  26.]],\n\n         ...,\n\n         [[199., 206., 199.],\n          [201. [Op:__inference_multi_step_on_iterator_414719]"
          ]
        }
      ],
      "source": [
        "# === Training cell ===\n",
        "# Paste & run this after you've defined all classes/functions (run_training, predict_video, evaluate_on_dataset)\n",
        "\n",
        "import time, json, csv\n",
        "from pathlib import Path\n",
        "\n",
        "# Config\n",
        "DATA_ROOT = \"/content/deepfake_dataset\"\n",
        "EPOCHS = 10         # set to 10 (or 8/you choose)\n",
        "BATCH_SIZE = 2\n",
        "RESUME = True       # resume from existing checkpoint if present\n",
        "SAVE_EVAL_JSON = \"/content/eval_results.json\"\n",
        "SAVE_EVAL_CSV  = \"/content/eval_per_video.csv\"\n",
        "\n",
        "print(\"Starting training run\")\n",
        "print(\"DATA_ROOT:\", DATA_ROOT, \"EPOCHS:\", EPOCHS, \"BATCH_SIZE:\", BATCH_SIZE, \"RESUME:\", RESUME)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Run training (returns detector, history, (train_p, val_p, train_l, val_l))\n",
        "detector, history, (train_p, val_p, train_l, val_l) = run_training(\n",
        "    base_dir=DATA_ROOT,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    resume=RESUME\n",
        ")\n",
        "\n",
        "print(f\"Training finished in {(time.time() - start_time)/60:.2f} minutes\")\n",
        "print(\"Best model saved at:\", MODEL_PATH)\n",
        "\n",
        "# Optional: quick history plot (if matplotlib available)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    if hasattr(history, \"history\"):\n",
        "        h = history.history\n",
        "        plt.figure(figsize=(10,4))\n",
        "        if \"loss\" in h:\n",
        "            plt.subplot(1,2,1); plt.plot(h.get(\"loss\",[]), label=\"train loss\"); plt.plot(h.get(\"val_loss\",[]), label=\"val loss\"); plt.legend(); plt.title(\"Loss\")\n",
        "        if \"video_pred_acc\" in h or \"val_video_pred_acc\" in h:\n",
        "            plt.subplot(1,2,2); plt.plot(h.get(\"video_pred_acc\", []), label=\"train acc\"); plt.plot(h.get(\"val_video_pred_acc\", []), label=\"val acc\"); plt.legend(); plt.title(\"Video Acc\")\n",
        "        plt.tight_layout(); plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Plotting skipped (matplotlib missing or error):\", e)\n",
        "\n",
        "# Evaluate on validation set using existing helper (will print per-video results)\n",
        "print(\"\\nRunning evaluation on validation samples...\")\n",
        "eval_results = {}\n",
        "try:\n",
        "    # evaluate_on_dataset will call predict_video internally and print summary\n",
        "    results = evaluate_on_dataset(detector, DATA_ROOT, num_samples=6)\n",
        "    # Convert to simpler metrics if possible\n",
        "    y_true = [r[1] for r in results]\n",
        "    y_scores = [r[2] for r in results]\n",
        "    from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "    preds = [1 if s>=0.5 else 0 for s in y_scores]\n",
        "    acc = accuracy_score(y_true, preds) if len(y_true)>0 else float(\"nan\")\n",
        "    auc = roc_auc_score(y_true, y_scores) if len(set(y_true))>1 else float(\"nan\")\n",
        "    cm = confusion_matrix(y_true, preds).tolist() if len(y_true)>0 else None\n",
        "    eval_results = {\"accuracy\": acc, \"auc\": auc, \"confusion_matrix\": cm, \"y_scores\": y_scores, \"video_paths\":[r[0] for r in results], \"y_true\": y_true}\n",
        "except Exception as e:\n",
        "    print(\"evaluate_on_dataset failed:\", e)\n",
        "    # fallback: run prediction across validation set (may be slower)\n",
        "    y_true, y_scores, vpaths = [], [], []\n",
        "    for vp, lbl in zip(val_p, val_l):\n",
        "        try:\n",
        "            score, _ = predict_video(detector, vp)\n",
        "            y_true.append(int(lbl)); y_scores.append(float(score)); vpaths.append(vp)\n",
        "        except Exception as ex:\n",
        "            print(\"Skipping\", vp, \"due to error:\", ex)\n",
        "    try:\n",
        "        from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "        preds = [1 if s>=0.5 else 0 for s in y_scores]\n",
        "        acc = accuracy_score(y_true, preds) if len(y_true)>0 else float(\"nan\")\n",
        "        auc = roc_auc_score(y_true, y_scores) if len(set(y_true))>1 else float(\"nan\")\n",
        "        cm = confusion_matrix(y_true, preds).tolist() if len(y_true)>0 else None\n",
        "        eval_results = {\"accuracy\": acc, \"auc\": auc, \"confusion_matrix\": cm, \"y_scores\": y_scores, \"video_paths\": vpaths, \"y_true\": y_true}\n",
        "    except Exception as e2:\n",
        "        print(\"Could not compute sklearn metrics:\", e2)\n",
        "        eval_results = {\"accuracy\": None, \"auc\": None, \"confusion_matrix\": None, \"y_scores\": y_scores, \"video_paths\": vpaths, \"y_true\": y_true}\n",
        "\n",
        "# Save evaluation summary JSON\n",
        "summary = {\"accuracy\": float(eval_results[\"accuracy\"]) if eval_results.get(\"accuracy\") is not None else None,\n",
        "           \"auc\": float(eval_results[\"auc\"]) if eval_results.get(\"auc\") is not None else None,\n",
        "           \"confusion_matrix\": eval_results.get(\"confusion_matrix\")}\n",
        "with open(SAVE_EVAL_JSON, \"w\") as f:\n",
        "    json.dump(summary, f)\n",
        "print(\"Saved evaluation summary to\", SAVE_EVAL_JSON)\n",
        "\n",
        "# Save per-video CSV\n",
        "with open(SAVE_EVAL_CSV, \"w\", newline=\"\") as csvf:\n",
        "    writer = csv.writer(csvf)\n",
        "    writer.writerow([\"video_path\",\"true_label\",\"score\",\"pred_label\"])\n",
        "    for vp, lbl, score in zip(eval_results.get(\"video_paths\", []), eval_results.get(\"y_true\", []), eval_results.get(\"y_scores\", [])):\n",
        "        writer.writerow([vp, int(lbl), float(score), int(score >= 0.5)])\n",
        "print(\"Saved per-video results to\", SAVE_EVAL_CSV)\n",
        "\n",
        "print(\"\\nDone.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8SdMxcFE8KkX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8SdMxcFE8KkX",
        "outputId": "3fdb4e18-b85e-4603-cb59-11e9ef84b6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 363 real videos, 1002 fake videos\n",
            "Warmup cache (this will process videos and can take time)...\n",
            "\n",
            "[Warmup] Caching 1092 train videos ...\n",
            "  Cached 50/1092 videos\n",
            "  Cached 100/1092 videos\n",
            "  Cached 150/1092 videos\n",
            "  Cached 200/1092 videos\n",
            "  Cached 250/1092 videos\n",
            "  Cached 300/1092 videos\n",
            "  Cached 350/1092 videos\n",
            "  Cached 400/1092 videos\n",
            "  Cached 450/1092 videos\n",
            "  Cached 500/1092 videos\n",
            "  Cached 550/1092 videos\n",
            "  Cached 600/1092 videos\n",
            "  Cached 650/1092 videos\n",
            "  Cached 700/1092 videos\n",
            "  Cached 750/1092 videos\n",
            "  Cached 800/1092 videos\n",
            "  Cached 850/1092 videos\n",
            "  Cached 900/1092 videos\n",
            "  Cached 950/1092 videos\n",
            "  Cached 1000/1092 videos\n",
            "  Cached 1050/1092 videos\n",
            "[Warmup] Done caching train\n",
            "\n",
            "\n",
            "[Warmup] Caching 273 val videos ...\n",
            "  Cached 50/273 videos\n",
            "  Cached 100/273 videos\n",
            "  Cached 150/273 videos\n",
            "  Cached 200/273 videos\n",
            "  Cached 250/273 videos\n",
            "[Warmup] Done caching val\n",
            "\n",
            "Using class weights: {'video_pred': {0: 0.6807980049875312, 1: 1.8827586206896552}}\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "TensorFlowTrainer.fit() got an unexpected keyword argument 'workers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1283160764.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_on_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_figures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3540201228.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(base_dir, epochs, batch_size, resume)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_video_pred_acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[0;32m---> 25\u001b[0;31m     history = det.model.fit(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TensorFlowTrainer.fit() got an unexpected keyword argument 'workers'"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    EPOCHS = 8\n",
        "    BATCH_SIZE = 2\n",
        "    detector, history, (train_p, val_p, train_l, val_l) = run_training(base_dir=DATA_ROOT, epochs=EPOCHS, batch_size=BATCH_SIZE, resume=True)\n",
        "    plot_history(history)\n",
        "    eval_results = evaluate_on_validation(detector, val_p, val_l, plot_figures=True)\n",
        "    # Save outputs\n",
        "    eval_summary = {\"accuracy\": float(eval_results[\"accuracy\"]), \"auc\": float(eval_results[\"auc\"]), \"confusion_matrix\": eval_results[\"confusion_matrix\"].tolist()}\n",
        "    with open(\"/content/eval_results.json\", \"w\") as f:\n",
        "        json.dump(eval_summary, f)\n",
        "    import csv\n",
        "    with open(\"/content/eval_per_video.csv\", \"w\", newline=\"\") as csvf:\n",
        "        writer = csv.writer(csvf)\n",
        "        writer.writerow([\"video_path\",\"true_label\",\"score\",\"pred_label\"])\n",
        "        for vp, lbl, score in zip(val_p, val_l, eval_results[\"y_scores\"]):\n",
        "            writer.writerow([vp, int(lbl), float(score), int(score>=0.5)])\n",
        "    print(\"Saved evaluation summary to /content/eval_results.json and /content/eval_per_video.csv\")\n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KkiR0mmZ8K50",
      "metadata": {
        "id": "KkiR0mmZ8K50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nQeVdXeO46e_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQeVdXeO46e_",
        "outputId": "918836bb-66e0-4433-8377-e5ce6dd24551"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected missing/incompatible packages or import error: ModuleNotFoundError(\"No module named 'cv2'\")\n",
            "Installing compatible numpy and opencv + supporting libs. Runtime will restart.\n"
          ]
        }
      ],
      "source": [
        "# === ALL-IN-ONE Colab cell: install, restart if needed, mount, extract, train, evaluate ===\n",
        "import os, sys, time, math, glob, shutil, zipfile, random, hashlib, pickle\n",
        "from pathlib import Path\n",
        "\n",
        "SETUP_FLAG = \"/content/.colab_setup_done\"\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Attempt imports; if they fail, install compatible packages and restart the runtime.\n",
        "# -------------------------------\n",
        "def ensure_env():\n",
        "    try:\n",
        "        import importlib\n",
        "        import numpy as np\n",
        "        import cv2\n",
        "        import tensorflow as tf\n",
        "        import sklearn\n",
        "        import matplotlib\n",
        "        # If we reach here, assume good environment.\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Detected missing/incompatible packages or import error:\", repr(e))\n",
        "        print(\"Installing compatible numpy and opencv + supporting libs. Runtime will restart.\")\n",
        "        # Uninstall/Install compatible versions commonly working in Colab\n",
        "        # (This cell will kill the runtime below; re-run the same cell after restart to continue.)\n",
        "        os.system(\"pip uninstall -y opencv-python opencv-python-headless >/dev/null 2>&1 || true\")\n",
        "        os.system(\"pip install -q --upgrade pip\")\n",
        "        # Choose numpy compatible with many TF & OpenCV wheels; adjust if TF version later indicates mismatch.\n",
        "        os.system(\"pip install -q numpy==1.25.2\")\n",
        "        os.system(\"pip install -q opencv-python-headless==4.7.0.72 scikit-learn matplotlib imageio-ffmpeg tqdm\")\n",
        "        # After install, create flag so that after restart we don't reinstall repeatedly\n",
        "        Path(SETUP_FLAG).write_text(\"installed\\n\")\n",
        "        # Restart runtime\n",
        "        print(\"Packages installed. Restarting runtime now to load native extensions...\")\n",
        "        os.kill(os.getpid(), 9)\n",
        "\n",
        "# If not already setup, try to ensure environment\n",
        "if not os.path.exists(SETUP_FLAG):\n",
        "    ensure_env()\n",
        "\n",
        "# After a restart, or if setup already done, proceed\n",
        "# -------------------------------\n",
        "# 2) Now safe imports and seeding\n",
        "# -------------------------------\n",
        "import numpy as np, cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Input, Model, callbacks\n",
        "from tensorflow.keras.applications import InceptionResNetV2, EfficientNetB4\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as irv2_preprocess\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "# reproducibility seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Mixed precision: enable if GPU and TF supports it\n",
        "try:\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "        print(\"Mixed precision enabled (mixed_float16).\")\n",
        "except Exception as e:\n",
        "    print(\"Mixed precision not enabled:\", e)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) Paths and Drive extraction\n",
        "# -------------------------------\n",
        "CACHE_DIR = \"/content/deepfake_cache\"\n",
        "MODEL_PATH = \"/content/deepfake_t4.h5\"\n",
        "DATA_ROOT = \"/content/deepfake_dataset\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "# Mount Drive if available (Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "    print(\"Google Drive mounted.\")\n",
        "except Exception as e:\n",
        "    DRIVE_ROOT = None\n",
        "    print(\"Google Drive mount skipped or not available:\", e)\n",
        "\n",
        "def find_and_extract(zip_name, target_subdir):\n",
        "    target_dir = os.path.join(DATA_ROOT, target_subdir)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    search_roots = []\n",
        "    if DRIVE_ROOT and os.path.exists(DRIVE_ROOT):\n",
        "        search_roots.append(DRIVE_ROOT)\n",
        "    search_roots.append(os.getcwd())\n",
        "    found = False\n",
        "    for root in search_roots:\n",
        "        for path, dirs, files in os.walk(root):\n",
        "            if zip_name in files:\n",
        "                zip_path = os.path.join(path, zip_name)\n",
        "                print(f\"Found {zip_name} at {zip_path}. Extracting .mp4 files to {target_dir} ...\")\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "                        for member in z.namelist():\n",
        "                            if member.lower().endswith('.mp4'):\n",
        "                                member_name = os.path.basename(member)\n",
        "                                if not member_name:\n",
        "                                    continue\n",
        "                                dest = os.path.join(target_dir, member_name)\n",
        "                                with z.open(member) as source, open(dest, \"wb\") as destf:\n",
        "                                    shutil.copyfileobj(source, destf)\n",
        "                    found = True\n",
        "                except zipfile.BadZipFile:\n",
        "                    print(f\"Error: {zip_path} is not a valid zip file.\")\n",
        "                break\n",
        "        if found:\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Warning: {zip_name} not found under {search_roots}. Please upload or place the zip in Drive/MyDrive or /content.\")\n",
        "    return found\n",
        "\n",
        "# Attempt extraction (r.zip -> real/real ; f.zip -> fake/fakes)\n",
        "find_and_extract(\"r.zip\", \"real/real\")\n",
        "find_and_extract(\"f.zip\", \"fake/fakes\")\n",
        "\n",
        "# Show counts\n",
        "real_count = len(glob.glob(os.path.join(DATA_ROOT, \"real\",\"real\",\"*.mp4\")))\n",
        "fake_count = len(glob.glob(os.path.join(DATA_ROOT, \"fake\",\"fakes\",\"*.mp4\")))\n",
        "print(f\"Dataset counts after extraction: real={real_count}, fake={fake_count}\")\n",
        "\n",
        "if real_count + fake_count == 0:\n",
        "    print(\"No videos found for training. Please ensure r.zip and f.zip exist and contain .mp4 files in Drive or upload them.\")\n",
        "    # Exit gracefully - nothing further to run\n",
        "    raise SystemExit(\"No dataset available.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Detector class (kept original logic; small defensive fixes)\n",
        "# -------------------------------\n",
        "class PatentAlignedDeepfakeDetector:\n",
        "    def __init__(self, max_frames=6, resize_to=(224,224), ascii_grid=(80,40),\n",
        "                 ascii_stats_dim=32, cache_dir=CACHE_DIR):\n",
        "        self.max_frames = max_frames\n",
        "        self.resize_to = resize_to\n",
        "        self.ascii_grid = ascii_grid\n",
        "        self.ascii_stats_dim = ascii_stats_dim\n",
        "        self.ASCII_CHARS = \"@%#*+=-:. \"\n",
        "        self.intensity_bins = np.linspace(0, 255, len(self.ASCII_CHARS))\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    def _cache_key(self, video_path):\n",
        "        return os.path.join(self.cache_dir, hashlib.md5(video_path.encode()).hexdigest()+\".pkl\")\n",
        "\n",
        "    def vectorized_ascii_conversion(self, frame):\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        H, W = gray.shape\n",
        "        gh, gw = self.ascii_grid\n",
        "        if gh <= 0 or gw <= 0:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "        crop_h, crop_w = (H//gh)*gh, (W//gw)*gw\n",
        "        gray = gray[:crop_h,:crop_w] if crop_h>0 and crop_w>0 else gray\n",
        "        if crop_h==0 or crop_w==0:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "        cell_h, cell_w = crop_h//gh, crop_w//gw\n",
        "        try:\n",
        "            cells = gray.reshape(gh,cell_h,gw,cell_w)\n",
        "            means = cells.mean(axis=(1,3))\n",
        "            ascii_idx = np.digitize(means, self.intensity_bins)-1\n",
        "            ascii_idx = np.clip(ascii_idx, 0, len(self.ASCII_CHARS)-1)\n",
        "            ascii_vals = np.array([ord(self.ASCII_CHARS[i]) for i in ascii_idx.flatten()]).reshape(ascii_idx.shape)\n",
        "            ascii_img = np.stack([ascii_vals]*3,axis=-1).astype(np.float32)\n",
        "            return cv2.resize(ascii_img, self.resize_to)\n",
        "        except Exception:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "\n",
        "    def compute_ascii_stats(self, ascii_frames):\n",
        "        ent = []\n",
        "        for f in ascii_frames:\n",
        "            flat = f[:,:,0].flatten()\n",
        "            if len(flat)<3:\n",
        "                ent.append((0.0,0.0))\n",
        "                continue\n",
        "            ent.append((float(np.std(flat)), float(np.mean(flat))))\n",
        "        if len(ent)==0:\n",
        "            packed = np.array([0.0,0.0])\n",
        "        else:\n",
        "            packed = np.mean(ent,axis=0)\n",
        "        reps = int(np.ceil(self.ascii_stats_dim/packed.size))\n",
        "        return np.tile(packed,reps)[:self.ascii_stats_dim].astype(np.float32)\n",
        "\n",
        "    def extract_frames_dual_path(self, video_path):\n",
        "        cache_file = self._cache_key(video_path)\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                return pickle.load(open(cache_file,\"rb\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        pixel, ascii_ = [], []\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        if total > self.max_frames:\n",
        "            idxs = np.linspace(0, total-1, self.max_frames, dtype=int)\n",
        "        elif total > 0:\n",
        "            idxs = range(0, min(total, self.max_frames))\n",
        "        else:\n",
        "            idxs = []\n",
        "        for i in idxs:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, f = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "            f = cv2.resize(f, self.resize_to)[:,:,::-1]\n",
        "            pixel.append(f)\n",
        "            ascii_.append(self.vectorized_ascii_conversion(f))\n",
        "        cap.release()\n",
        "        while len(pixel) < self.max_frames:\n",
        "            if pixel:\n",
        "                pixel.append(pixel[-1].copy())\n",
        "                ascii_.append(ascii_[-1].copy())\n",
        "            else:\n",
        "                pixel.append(np.zeros((*self.resize_to,3), np.uint8))\n",
        "                ascii_.append(np.zeros((*self.resize_to,3), np.uint8))\n",
        "        pixel = np.stack(pixel).astype(np.float32)\n",
        "        ascii_ = np.stack(ascii_).astype(np.float32)\n",
        "        stats = self.compute_ascii_stats(ascii_)\n",
        "        data = (pixel, ascii_, stats)\n",
        "        try:\n",
        "            pickle.dump(data, open(cache_file, \"wb\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return data\n",
        "\n",
        "    def build_patent_aligned_model(self):\n",
        "        T, H, W = self.max_frames, *self.resize_to\n",
        "        pix_in = Input((T,H,W,3), name=\"pixel_input\")\n",
        "        asc_in = Input((T,H,W,3), name=\"ascii_input\")\n",
        "        stats_in = Input((self.ascii_stats_dim,), name=\"ascii_stats_input\")\n",
        "\n",
        "        irv2 = InceptionResNetV2(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(H,W,3))\n",
        "        effb4 = EfficientNetB4(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(H,W,3))\n",
        "        irv2.trainable = False\n",
        "        effb4.trainable = False\n",
        "\n",
        "        pix = layers.TimeDistributed(layers.Lambda(lambda x: irv2(irv2_preprocess(x))))(pix_in)\n",
        "        asc = layers.TimeDistributed(layers.Lambda(lambda x: effb4(eff_preprocess(x))))(asc_in)\n",
        "        stats = layers.RepeatVector(T)(layers.Dense(32, activation=\"relu\")(stats_in))\n",
        "        fused = layers.Concatenate()([pix, asc, stats])\n",
        "\n",
        "        temporal = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(fused)\n",
        "        frame_logits = layers.TimeDistributed(layers.Dense(1, activation=\"sigmoid\"), name=\"frame_logits\")(temporal)\n",
        "        video_pred = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name=\"video_pred\")(frame_logits)\n",
        "\n",
        "        self.model = Model([pix_in, asc_in, stats_in], [video_pred, frame_logits])\n",
        "        return self.model\n",
        "\n",
        "    def _temporal_penalty_loss(self, y_true, frame_logits):\n",
        "        diffs = frame_logits[:,1:,:] - frame_logits[:,:-1,:]\n",
        "        return tf.reduce_mean(tf.abs(diffs))\n",
        "\n",
        "    def compile_model(self, lr=1e-4, temporal_loss_weight=0.1):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy()\n",
        "        def vloss(y_true, y_pred): return bce(y_true, y_pred)\n",
        "        def tloss(y_true, y_pred): return self._temporal_penalty_loss(y_true, y_pred)\n",
        "        self.model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "            loss={\"video_pred\": vloss, \"frame_logits\": tloss},\n",
        "            loss_weights={\"video_pred\": 1.0, \"frame_logits\": temporal_loss_weight},\n",
        "            metrics={\"video_pred\": [tf.keras.metrics.BinaryAccuracy(name=\"acc\")]}\n",
        "        )\n",
        "\n",
        "    def dual_path_generator(self, paths, labels, batch_size=2, augment=False):\n",
        "        while True:\n",
        "            data = list(zip(paths, labels)); random.shuffle(data)\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch = data[i:i+batch_size]\n",
        "                Xp, Xa, Xs, Y = [], [], [], []\n",
        "                for vp, lbl in batch:\n",
        "                    pf, af, st = self.extract_frames_dual_path(vp)\n",
        "                    if augment and random.random() > 0.5:\n",
        "                        pf, af = pf[:, ::-1, :, :], af[:, ::-1, :, :]\n",
        "                    Xp.append(pf.astype(\"float32\"))\n",
        "                    Xa.append(af.astype(\"float32\"))\n",
        "                    Xs.append(st.astype(\"float32\"))\n",
        "                    Y.append(lbl)\n",
        "                Xp = np.stack(Xp)\n",
        "                Xa = np.stack(Xa)\n",
        "                Xs = np.stack(Xs)\n",
        "                Yv = np.array(Y, dtype=np.float32).reshape(-1,1)\n",
        "                Yf = np.zeros((len(Y), self.max_frames, 1), dtype=np.float32)\n",
        "                yield [Xp, Xa, Xs], {\"video_pred\": Yv, \"frame_logits\": Yf}\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Dataset loader and helpers\n",
        "# -------------------------------\n",
        "def load_dataset(base_dir=DATA_ROOT, test_size=0.2):\n",
        "    real_dir = os.path.join(base_dir, \"real\", \"real\")\n",
        "    fake_dir = os.path.join(base_dir, \"fake\", \"fakes\")\n",
        "    real = sorted(glob.glob(os.path.join(real_dir, \"*.mp4\")))\n",
        "    fake = sorted(glob.glob(os.path.join(fake_dir, \"*.mp4\")))\n",
        "    print(f\"Found {len(real)} real videos, {len(fake)} fake videos\")\n",
        "    if len(real) + len(fake) == 0:\n",
        "        raise FileNotFoundError(\"No mp4 files found.\")\n",
        "    paths = real + fake\n",
        "    labels = [1]*len(real) + [0]*len(fake)\n",
        "    strat = labels if len(set(labels))>1 else None\n",
        "    return train_test_split(paths, labels, test_size=test_size, stratify=strat, random_state=SEED)\n",
        "\n",
        "def warmup_cache(detector, paths, label=\"train\"):\n",
        "    print(f\"\\n[Warmup] Caching {len(paths)} {label} videos ...\")\n",
        "    for idx, vp in enumerate(paths):\n",
        "        try:\n",
        "            _ = detector.extract_frames_dual_path(vp)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed caching {vp}: {e}\")\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"  Cached {idx+1}/{len(paths)} videos\")\n",
        "    print(f\"[Warmup] Done caching {label} set (stored in {detector.cache_dir})\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Training, evaluation and plotting\n",
        "# -------------------------------\n",
        "def plot_history(history):\n",
        "    hist = history.history\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    if \"loss\" in hist:\n",
        "        plt.plot(hist[\"loss\"], label=\"train_total_loss\")\n",
        "    if \"val_loss\" in hist:\n",
        "        plt.plot(hist[\"val_loss\"], label=\"val_total_loss\")\n",
        "    if \"video_pred_loss\" in hist:\n",
        "        plt.plot(hist[\"video_pred_loss\"], label=\"train_video_loss\")\n",
        "    if \"val_video_pred_loss\" in hist:\n",
        "        plt.plot(hist[\"val_video_pred_loss\"], label=\"val_video_loss\")\n",
        "    plt.title(\"Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    if \"video_pred_acc\" in hist:\n",
        "        plt.plot(hist[\"video_pred_acc\"], label=\"train_video_acc\")\n",
        "    if \"val_video_pred_acc\" in hist:\n",
        "        plt.plot(hist[\"val_video_pred_acc\"], label=\"val_video_acc\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def predict_single(detector, video_path):\n",
        "    pixel_frames, ascii_frames, stats = detector.extract_frames_dual_path(video_path)\n",
        "    Xp = np.expand_dims(pixel_frames.astype(\"float32\"), 0)\n",
        "    Xa = np.expand_dims(ascii_frames.astype(\"float32\"), 0)\n",
        "    Xs = np.expand_dims(stats.astype(\"float32\"), 0)\n",
        "    preds = detector.model.predict([Xp, Xa, Xs], verbose=0)\n",
        "    video_pred, frame_preds = preds\n",
        "    video_score = float(video_pred[0][0])\n",
        "    frame_scores = frame_preds[0,:,0]\n",
        "    return video_score, frame_scores\n",
        "\n",
        "def evaluate_on_validation(detector, val_paths, val_labels, plot_figures=True):\n",
        "    y_true, y_scores = [], []\n",
        "    for vp, lbl in zip(val_paths, val_labels):\n",
        "        try:\n",
        "            score, _ = predict_single(detector, vp)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] predict failed for {vp}: {e}\")\n",
        "            score = 0.5\n",
        "        y_true.append(lbl)\n",
        "        y_scores.append(score)\n",
        "    y_true = np.array(y_true)\n",
        "    y_scores = np.array(y_scores)\n",
        "    y_pred = (y_scores >= 0.5).astype(int)\n",
        "    try:\n",
        "        auc_score = roc_auc_score(y_true, y_scores)\n",
        "    except Exception:\n",
        "        auc_score = float('nan')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cr = classification_report(y_true, y_pred, digits=4)\n",
        "    acc = (y_pred == y_true).mean()\n",
        "    print(\"\\n=== Evaluation on validation set ===\")\n",
        "    print(f\"Samples: {len(y_true)}  Accuracy: {acc:.4f}  AUC: {auc_score:.4f}\")\n",
        "    print(\"\\nClassification report:\\n\", cr)\n",
        "    print(\"\\nConfusion matrix:\\n\", cm)\n",
        "    if plot_figures:\n",
        "        try:\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "            roc_auc_val = auc(fpr, tpr)\n",
        "            plt.figure(figsize=(6,5))\n",
        "            plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_val:.4f}\")\n",
        "            plt.plot([0,1],[0,1],\"--\")\n",
        "            plt.title(\"ROC curve\")\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(\"ROC plot failed:\", e)\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion matrix\")\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(2)\n",
        "        plt.xticks(tick_marks, [\"fake(0)\",\"real(1)\"])\n",
        "        plt.yticks(tick_marks, [\"fake(0)\",\"real(1)\"])\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    return {\"accuracy\": acc, \"auc\": auc_score, \"confusion_matrix\": cm, \"report\": cr, \"y_true\": y_true, \"y_scores\": y_scores}\n",
        "\n",
        "# -------------------------------\n",
        "# 7) Main training entry\n",
        "# -------------------------------\n",
        "def run_training(base_dir=DATA_ROOT, epochs=8, batch_size=2, resume=True):\n",
        "    train_p, val_p, train_l, val_l = load_dataset(base_dir)\n",
        "    det = PatentAlignedDeepfakeDetector(cache_dir=CACHE_DIR)\n",
        "    det_model = det.build_patent_aligned_model()\n",
        "    det.compile_model()\n",
        "\n",
        "    # Warmup cache (comment out if dataset very large)\n",
        "    print(\"Starting cache warmup. This may take some time depending on dataset size.\")\n",
        "    warmup_cache(det, train_p, label=\"train\")\n",
        "    warmup_cache(det, val_p, label=\"val\")\n",
        "\n",
        "    # Compute class weights (balanced)\n",
        "    labels_all = np.array(train_l)\n",
        "    classes = np.unique(labels_all)\n",
        "    cw = compute_class_weight(class_weight='balanced', classes=classes, y=labels_all)\n",
        "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, cw)}\n",
        "    # Keras multi-output class_weight requires mapping output name -> dict\n",
        "    keras_class_weight = {\"video_pred\": class_weight_dict}\n",
        "    print(\"Computed class weights for training video_pred output:\", keras_class_weight)\n",
        "\n",
        "    train_gen = det.dual_path_generator(train_p, train_l, batch_size=batch_size, augment=True)\n",
        "    val_gen = det.dual_path_generator(val_p, val_l, batch_size=batch_size, augment=False)\n",
        "\n",
        "    steps = max(1, math.ceil(len(train_p) / batch_size))\n",
        "    val_steps = max(1, math.ceil(len(val_p) / batch_size))\n",
        "\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor=\"val_video_pred_acc\", verbose=1),\n",
        "        callbacks.EarlyStopping(monitor=\"val_video_pred_acc\", patience=3, restore_best_weights=True, verbose=1),\n",
        "        callbacks.ReduceLROnPlateau(monitor=\"val_video_pred_acc\", factor=0.5, patience=2, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = det.model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=cb,\n",
        "        workers=1,\n",
        "        use_multiprocessing=False,\n",
        "        class_weight=keras_class_weight\n",
        "    )\n",
        "\n",
        "    # Ensure best weights loaded\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        det.model.load_weights(MODEL_PATH)\n",
        "\n",
        "    print(\"Training finished. Model saved to:\", MODEL_PATH)\n",
        "    return det, history, (train_p, val_p, train_l, val_l)\n",
        "\n",
        "# -------------------------------\n",
        "# 8) Execute training + evaluation\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # User-adjustable parameters\n",
        "    EPOCHS = 8            # increase if resources/time permit\n",
        "    BATCH_SIZE = 2        # increase if GPU memory allows\n",
        "\n",
        "    detector, history, (train_p, val_p, train_l, val_l) = run_training(base_dir=DATA_ROOT, epochs=EPOCHS, batch_size=BATCH_SIZE, resume=True)\n",
        "\n",
        "    # Plot curves\n",
        "    plot_history(history)\n",
        "\n",
        "    # Evaluate on full validation set\n",
        "    eval_results = evaluate_on_validation(detector, val_p, val_l, plot_figures=True)\n",
        "\n",
        "    # Save evaluation and per-video scores\n",
        "    eval_summary = {\n",
        "        \"accuracy\": float(eval_results[\"accuracy\"]),\n",
        "        \"auc\": float(eval_results[\"auc\"]),\n",
        "        \"confusion_matrix\": eval_results[\"confusion_matrix\"].tolist()\n",
        "    }\n",
        "    with open(\"/content/eval_results.json\", \"w\") as f:\n",
        "        json.dump(eval_summary, f)\n",
        "    # Save per-video CSV\n",
        "    import csv\n",
        "    with open(\"/content/eval_per_video.csv\", \"w\", newline=\"\") as csvf:\n",
        "        writer = csv.writer(csvf)\n",
        "        writer.writerow([\"video_path\",\"true_label\",\"score\",\"pred_label\"])\n",
        "        for vp, lbl, score in zip(val_p, val_l, eval_results[\"y_scores\"]):\n",
        "            writer.writerow([vp, int(lbl), float(score), int(score>=0.5)])\n",
        "    print(\"Saved evaluation summary to /content/eval_results.json and per-video scores to /content/eval_per_video.csv\")\n",
        "    print(\"Finished all operations.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2YB1BMSP7kkA",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YB1BMSP7kkA",
        "outputId": "632ca6f3-8d4e-488e-e051-5764d9790a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Found r.zip at /content/drive/MyDrive/r.zip. Extracting to /content/deepfake_dataset/real/real ...\n",
            "Found f.zip at /content/drive/MyDrive/f.zip. Extracting to /content/deepfake_dataset/fake/fakes ...\n",
            "After extraction, video counts:\n",
            " real: 363\n",
            " fake: 1002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1173429031.py\", line 74, in <cell line: 0>\n",
            "    import numpy as np, cv2\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\", line 54, in create_module\n",
            "    loader.exec_module(module)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\", line 181, in <module>\n",
            "    bootstrap()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\", line 153, in bootstrap\n",
            "    native_module = importlib.import_module(\"cv2\")\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\", line 51, in create_module\n",
            "    module = importlib.util.module_from_spec(spec)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1173429031.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# === 1. Imports, GPU & Mixed precision ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_code_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreviously_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0mbootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cv2/__init__.py\u001b[0m in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mpy_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mnative_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cv2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py\u001b[0m in \u001b[0;36mcreate_module\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Colab-ready updated notebook\n",
        "# - Mounts Google Drive and unzips r.zip (real) and f.zip (fake) into the expected folder layout:\n",
        "#       /content/deepfake_dataset/real/real/*.mp4\n",
        "#       /content/deepfake_dataset/fake/fakes/*.mp4\n",
        "# - Keeps original model logic and architecture\n",
        "# - Adds evaluation: confusion matrix, classification report, ROC + AUC, plots, and history display\n",
        "# - Makes training robust to small datasets and guarantees at least 1 step per epoch\n",
        "\n",
        "# === 0. Environment & Drive setup ===\n",
        "import os, glob, zipfile, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive (runs interactively in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "except Exception as e:\n",
        "    DRIVE_ROOT = None\n",
        "    print(\"Warning: google.colab.drive not available (not running in Colab?). Continue assuming dataset is on /content or user will upload manually.\")\n",
        "\n",
        "def find_and_extract(zip_name, target_subdir):\n",
        "    \"\"\"\n",
        "    Search drive for zip_name (e.g., 'r.zip') and extract into /content/deepfake_dataset/<target_subdir>.\n",
        "    target_subdir examples: 'real/real' or 'fake/fakes'\n",
        "    \"\"\"\n",
        "    out_base = \"/content/deepfake_dataset\"\n",
        "    target_dir = os.path.join(out_base, target_subdir)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    # search in drive if mounted, else in current working dir\n",
        "    search_roots = []\n",
        "    if DRIVE_ROOT and os.path.exists(DRIVE_ROOT):\n",
        "        search_roots.append(DRIVE_ROOT)\n",
        "    search_roots.append(os.getcwd())\n",
        "    found = False\n",
        "    for root in search_roots:\n",
        "        for path, dirs, files in os.walk(root):\n",
        "            if zip_name in files:\n",
        "                zip_path = os.path.join(path, zip_name)\n",
        "                print(f\"Found {zip_name} at {zip_path}. Extracting to {target_dir} ...\")\n",
        "                try:\n",
        "                    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "                        # extract only mp4s into target_dir preserving file names\n",
        "                        for member in z.namelist():\n",
        "                            if member.lower().endswith('.mp4'):\n",
        "                                # sanitize member name and write\n",
        "                                member_name = os.path.basename(member)\n",
        "                                if not member_name:\n",
        "                                    continue\n",
        "                                dest = os.path.join(target_dir, member_name)\n",
        "                                with z.open(member) as source, open(dest, \"wb\") as destf:\n",
        "                                    shutil.copyfileobj(source, destf)\n",
        "                    found = True\n",
        "                except zipfile.BadZipFile:\n",
        "                    print(f\"Error: {zip_path} is not a valid zip file.\")\n",
        "                break\n",
        "        if found:\n",
        "            break\n",
        "    if not found:\n",
        "        print(f\"Warning: {zip_name} not found under {search_roots}. Please upload or place the zip in your Drive or /content.\")\n",
        "\n",
        "# Extract r.zip -> real/real and f.zip -> fake/fakes\n",
        "find_and_extract(\"r.zip\", \"real/real\")\n",
        "find_and_extract(\"f.zip\", \"fake/fakes\")\n",
        "\n",
        "# Print counts\n",
        "print(\"After extraction, video counts:\")\n",
        "print(\" real:\", len(glob.glob(\"/content/deepfake_dataset/real/real/*.mp4\")))\n",
        "print(\" fake:\", len(glob.glob(\"/content/deepfake_dataset/fake/fakes/*.mp4\")))\n",
        "\n",
        "\n",
        "# === 1. Imports, GPU & Mixed precision ===\n",
        "import random, pickle, hashlib\n",
        "import numpy as np, cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Input, Model, callbacks\n",
        "from tensorflow.keras.applications import InceptionResNetV2, EfficientNetB4\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as irv2_preprocess\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess\n",
        "\n",
        "# sklearn & plotting for evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "# GPU visibility\n",
        "print(\"GPUs visible:\", tf.config.list_physical_devices('GPU'))\n",
        "# Mixed precision (safe setup)\n",
        "try:\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "    print(\"Mixed precision policy set to mixed_float16\")\n",
        "except Exception as e:\n",
        "    print(\"Mixed precision unavailable or failed to set:\", e)\n",
        "\n",
        "# === 2. Config paths & parameters ===\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1gyCRKGC6FQg",
      "metadata": {
        "id": "1gyCRKGC6FQg"
      },
      "outputs": [],
      "source": [
        "CACHE_DIR = \"/content/deepfake_cache\"\n",
        "MODEL_PATH = \"/content/deepfake_t4.h5\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# === 3. Detector class (kept logic; minor safety fixes) ===\n",
        "class PatentAlignedDeepfakeDetector:\n",
        "    def __init__(self, max_frames=6, resize_to=(224,224), ascii_grid=(80,40),\n",
        "                 ascii_stats_dim=32, cache_dir=CACHE_DIR):\n",
        "        self.max_frames = max_frames\n",
        "        self.resize_to = resize_to\n",
        "        self.ascii_grid = ascii_grid\n",
        "        self.ascii_stats_dim = ascii_stats_dim\n",
        "        self.ASCII_CHARS = \"@%#*+=-:. \"\n",
        "        self.intensity_bins = np.linspace(0, 255, len(self.ASCII_CHARS))\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    def _cache_key(self, video_path):\n",
        "        return os.path.join(self.cache_dir, hashlib.md5(video_path.encode()).hexdigest()+\".pkl\")\n",
        "\n",
        "    def vectorized_ascii_conversion(self, frame):\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        H, W = gray.shape\n",
        "        gh, gw = self.ascii_grid\n",
        "        # guard against tiny frames\n",
        "        if gh <= 0 or gw <= 0:\n",
        "            return cv2.resize(np.stack([np.zeros((H,W),np.uint8)]*3,axis=-1).astype(np.float32), self.resize_to)\n",
        "        crop_h, crop_w = (H//gh)*gh, (W//gw)*gw\n",
        "        gray = gray[:crop_h,:crop_w] if crop_h>0 and crop_w>0 else gray\n",
        "        # if grid is too coarse, fallback to resized grayscale repeat\n",
        "        if crop_h==0 or crop_w==0:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "        cell_h, cell_w = crop_h//gh, crop_w//gw\n",
        "        try:\n",
        "            cells = gray.reshape(gh,cell_h,gw,cell_w)\n",
        "            means = cells.mean(axis=(1,3))\n",
        "            ascii_idx = np.digitize(means, self.intensity_bins)-1\n",
        "            ascii_idx = np.clip(ascii_idx, 0, len(self.ASCII_CHARS)-1)\n",
        "            ascii_vals = np.array([ord(self.ASCII_CHARS[i]) for i in ascii_idx.flatten()]).reshape(ascii_idx.shape)\n",
        "            ascii_img = np.stack([ascii_vals]*3,axis=-1).astype(np.float32)\n",
        "            return cv2.resize(ascii_img, self.resize_to)\n",
        "        except Exception:\n",
        "            tmp = cv2.resize(gray, self.resize_to)\n",
        "            return np.stack([tmp,tmp,tmp],axis=-1).astype(np.float32)\n",
        "\n",
        "    def compute_ascii_stats(self, ascii_frames):\n",
        "        ent = []\n",
        "        for f in ascii_frames:\n",
        "            flat = f[:,:,0].flatten()\n",
        "            if len(flat)<3:\n",
        "                ent.append((0.0,0.0))\n",
        "                continue\n",
        "            ent.append((float(np.std(flat)), float(np.mean(flat))))\n",
        "        if len(ent)==0:\n",
        "            packed = np.array([0.0,0.0])\n",
        "        else:\n",
        "            packed = np.mean(ent,axis=0)\n",
        "        reps = int(np.ceil(self.ascii_stats_dim/packed.size))\n",
        "        return np.tile(packed,reps)[:self.ascii_stats_dim].astype(np.float32)\n",
        "\n",
        "    def extract_frames_dual_path(self, video_path):\n",
        "        cache_file = self._cache_key(video_path)\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                return pickle.load(open(cache_file,\"rb\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        pixel, ascii_ = [], []\n",
        "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "        # handle total==0 gracefully\n",
        "        if total > self.max_frames:\n",
        "            idxs = np.linspace(0, total-1, self.max_frames, dtype=int)\n",
        "        elif total > 0:\n",
        "            idxs = range(0, min(total, self.max_frames))\n",
        "        else:\n",
        "            idxs = []\n",
        "\n",
        "        for i in idxs:\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, f = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "            # opencv returns BGR; convert to RGB and resize\n",
        "            f = cv2.resize(f, self.resize_to)[:, :, ::-1]\n",
        "            pixel.append(f)\n",
        "            ascii_.append(self.vectorized_ascii_conversion(f))\n",
        "        cap.release()\n",
        "\n",
        "        # pad if not enough frames\n",
        "        while len(pixel) < self.max_frames:\n",
        "            if pixel:\n",
        "                pixel.append(pixel[-1].copy())\n",
        "                ascii_.append(ascii_[-1].copy())\n",
        "            else:\n",
        "                pixel.append(np.zeros((*self.resize_to,3), np.uint8))\n",
        "                ascii_.append(np.zeros((*self.resize_to,3), np.uint8))\n",
        "\n",
        "        pixel = np.stack(pixel).astype(np.float32)\n",
        "        ascii_ = np.stack(ascii_).astype(np.float32)\n",
        "        stats = self.compute_ascii_stats(ascii_)\n",
        "        data = (pixel, ascii_, stats)\n",
        "        try:\n",
        "            pickle.dump(data, open(cache_file, \"wb\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return data\n",
        "\n",
        "    def build_patent_aligned_model(self):\n",
        "        T, H, W = self.max_frames, *self.resize_to\n",
        "        pix_in = Input((T,H,W,3), name=\"pixel_input\")\n",
        "        asc_in = Input((T,H,W,3), name=\"ascii_input\")\n",
        "        stats_in = Input((self.ascii_stats_dim,), name=\"ascii_stats_input\")\n",
        "\n",
        "        # Backbones (pretrained)\n",
        "        irv2 = InceptionResNetV2(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(H,W,3))\n",
        "        effb4 = EfficientNetB4(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(H,W,3))\n",
        "        # keep both not trainable (this matches the effect of the original code)\n",
        "        effb4.trainable = False\n",
        "        irv2.trainable = False\n",
        "\n",
        "        pix = layers.TimeDistributed(layers.Lambda(lambda x: irv2(irv2_preprocess(x))))(pix_in)\n",
        "        asc = layers.TimeDistributed(layers.Lambda(lambda x: effb4(eff_preprocess(x))))(asc_in)\n",
        "        stats = layers.RepeatVector(T)(layers.Dense(32, activation=\"relu\")(stats_in))\n",
        "        fused = layers.Concatenate()([pix, asc, stats])\n",
        "\n",
        "        temporal = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(fused)\n",
        "        frame_logits = layers.TimeDistributed(layers.Dense(1, activation=\"sigmoid\"), name=\"frame_logits\")(temporal)\n",
        "        video_pred = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name=\"video_pred\")(frame_logits)\n",
        "\n",
        "        self.model = Model([pix_in, asc_in, stats_in], [video_pred, frame_logits])\n",
        "        return self.model\n",
        "\n",
        "    def _temporal_penalty_loss(self, y_true, frame_logits):\n",
        "        diffs = frame_logits[:,1:,:] - frame_logits[:,:-1,:]\n",
        "        return tf.reduce_mean(tf.abs(diffs))\n",
        "\n",
        "    def compile_model(self, lr=1e-4, temporal_loss_weight=0.1):\n",
        "        bce = tf.keras.losses.BinaryCrossentropy()\n",
        "        def vloss(y_true, y_pred): return bce(y_true, y_pred)\n",
        "        def tloss(y_true, y_pred): return self._temporal_penalty_loss(y_true, y_pred)\n",
        "        # Using \"video_pred\" and \"frame_logits\" losses as in original\n",
        "        self.model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "            loss={\"video_pred\": vloss, \"frame_logits\": tloss},\n",
        "            loss_weights={\"video_pred\": 1.0, \"frame_logits\": temporal_loss_weight},\n",
        "            metrics={\"video_pred\": [tf.keras.metrics.BinaryAccuracy(name=\"acc\")]}\n",
        "        )\n",
        "\n",
        "    def dual_path_generator(self, paths, labels, batch_size=2, augment=False):\n",
        "        while True:\n",
        "            data = list(zip(paths, labels)); random.shuffle(data)\n",
        "            for i in range(0, len(data), batch_size):\n",
        "                batch = data[i:i+batch_size]\n",
        "                Xp, Xa, Xs, Y = [], [], [], []\n",
        "                for vp, lbl in batch:\n",
        "                    pf, af, st = self.extract_frames_dual_path(vp)\n",
        "                    if augment and random.random() > 0.5:\n",
        "                        pf, af = pf[:, ::-1, :, :], af[:, ::-1, :, :]\n",
        "                    Xp.append(pf.astype(\"float32\"))\n",
        "                    Xa.append(af.astype(\"float32\"))\n",
        "                    Xs.append(st.astype(\"float32\"))\n",
        "                    Y.append(lbl)\n",
        "                Xp = np.stack(Xp)\n",
        "                Xa = np.stack(Xa)\n",
        "                Xs = np.stack(Xs)\n",
        "                Yv = np.array(Y, dtype=np.float32).reshape(-1,1)\n",
        "                Yf = np.zeros((len(Y), self.max_frames, 1), dtype=np.float32)\n",
        "                yield [Xp, Xa, Xs], {\"video_pred\": Yv, \"frame_logits\": Yf}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1rm1BPAB6acr",
      "metadata": {
        "id": "1rm1BPAB6acr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 4. Dataset loader (uses the unzipped layout) ===\n",
        "def load_dataset(base_dir=\"/content/deepfake_dataset\", test_size=0.2):\n",
        "    real_dir = os.path.join(base_dir, \"real\", \"real\")\n",
        "    fake_dir = os.path.join(base_dir, \"fake\", \"fakes\")\n",
        "    real = sorted(glob.glob(os.path.join(real_dir, \"*.mp4\")))\n",
        "    fake = sorted(glob.glob(os.path.join(fake_dir, \"*.mp4\")))\n",
        "    print(f\"Found {len(real)} real videos, {len(fake)} fake videos\")\n",
        "    if len(real) + len(fake) == 0:\n",
        "        raise FileNotFoundError(f\"No mp4 files found under {real_dir} or {fake_dir}. Please check extraction.\")\n",
        "    paths = real + fake\n",
        "    labels = [1] * len(real) + [0] * len(fake)\n",
        "    return train_test_split(paths, labels, test_size=test_size, stratify=labels if len(set(labels))>1 else None, random_state=42)\n",
        "\n",
        "\n",
        "# === 5. Cache warmup (optional but helpful) ===\n",
        "def warmup_cache(detector, paths, label=\"train\"):\n",
        "    print(f\"\\n[Warmup] Caching {len(paths)} {label} videos ...\")\n",
        "    for idx, vp in enumerate(paths):\n",
        "        try:\n",
        "            _ = detector.extract_frames_dual_path(vp)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed caching {vp}: {e}\")\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"  Cached {idx+1}/{len(paths)} videos\")\n",
        "    print(f\"[Warmup] Done caching {label} set ✅ (stored in {detector.cache_dir})\\n\")\n",
        "\n",
        "\n",
        "# === 6. Training loop with safe step calculation and callbacks ===\n",
        "def train(base_dir=\"/content/deepfake_dataset\", epochs=8, batch_size=2):\n",
        "    train_p, val_p, train_l, val_l = load_dataset(base_dir)\n",
        "    det = PatentAlignedDeepfakeDetector(cache_dir=CACHE_DIR)\n",
        "    model = det.build_patent_aligned_model()\n",
        "    det.compile_model()\n",
        "\n",
        "    # Warmup caches (optional; can be commented out for very large datasets)\n",
        "    warmup_cache(det, train_p, label=\"train\")\n",
        "    warmup_cache(det, val_p, label=\"val\")\n",
        "\n",
        "    train_gen = det.dual_path_generator(train_p, train_l, batch_size=batch_size, augment=True)\n",
        "    val_gen = det.dual_path_generator(val_p, val_l, batch_size=batch_size, augment=False)\n",
        "\n",
        "    steps = max(1, int(np.ceil(len(train_p) / batch_size)))\n",
        "    val_steps = max(1, int(np.ceil(len(val_p) / batch_size)))\n",
        "\n",
        "    cb = [\n",
        "        callbacks.ModelCheckpoint(MODEL_PATH, save_best_only=True,\n",
        "                                  monitor=\"val_video_pred_acc\", verbose=1),\n",
        "        callbacks.EarlyStopping(monitor=\"val_video_pred_acc\", patience=3,\n",
        "                                restore_best_weights=True, verbose=1),\n",
        "        callbacks.ReduceLROnPlateau(monitor=\"val_video_pred_acc\", factor=0.5,\n",
        "                                    patience=2, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps,\n",
        "        validation_steps=val_steps,\n",
        "        callbacks=cb,\n",
        "        workers=1,\n",
        "        use_multiprocessing=False\n",
        "    )\n",
        "\n",
        "    # After training, ensure best weights are loaded (ModelCheckpoint + EarlyStopping restore)\n",
        "    if os.path.exists(MODEL_PATH):\n",
        "        det.model.load_weights(MODEL_PATH)\n",
        "\n",
        "    print(f\"\\n✅ Training complete. Best model saved at: {MODEL_PATH}\")\n",
        "    return det, history, (train_p, val_p, train_l, val_l)\n",
        "\n",
        "\n",
        "# === 7. Prediction utilities & full evaluation on validation set ===\n",
        "def predict_single(detector, video_path):\n",
        "    pixel_frames, ascii_frames, stats = detector.extract_frames_dual_path(video_path)\n",
        "    Xp = np.expand_dims(pixel_frames.astype(\"float32\"), 0)\n",
        "    Xa = np.expand_dims(ascii_frames.astype(\"float32\"), 0)\n",
        "    Xs = np.expand_dims(stats.astype(\"float32\"), 0)\n",
        "    preds = detector.model.predict([Xp, Xa, Xs], verbose=0)\n",
        "    video_pred, frame_preds = preds\n",
        "    video_score = float(video_pred[0][0])\n",
        "    frame_scores = frame_preds[0,:,0]\n",
        "    return video_score, frame_scores\n",
        "\n",
        "def evaluate_on_validation(detector, val_paths, val_labels, plot_figures=True):\n",
        "    y_true = []\n",
        "    y_scores = []\n",
        "    for vp, lbl in zip(val_paths, val_labels):\n",
        "        try:\n",
        "            score, _ = predict_single(detector, vp)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] predict failed for {vp}: {e}\")\n",
        "            score = 0.5\n",
        "        y_true.append(lbl)\n",
        "        y_scores.append(score)\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_scores = np.array(y_scores)\n",
        "    y_pred = (y_scores >= 0.5).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    try:\n",
        "        auc_score = roc_auc_score(y_true, y_scores)\n",
        "    except Exception:\n",
        "        auc_score = float('nan')\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cr = classification_report(y_true, y_pred, digits=4)\n",
        "    acc = (y_pred == y_true).mean()\n",
        "\n",
        "    print(\"\\n=== Evaluation on validation set ===\")\n",
        "    print(f\"Samples: {len(y_true)}  Accuracy: {acc:.4f}  AUC: {auc_score:.4f}\")\n",
        "    print(\"\\nClassification report:\\n\", cr)\n",
        "    print(\"\\nConfusion matrix:\\n\", cm)\n",
        "\n",
        "    if plot_figures:\n",
        "        # Plot ROC\n",
        "        try:\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "            roc_auc_val = auc(fpr, tpr)\n",
        "            plt.figure(figsize=(6,5))\n",
        "            plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_val:.4f}\")\n",
        "            plt.plot([0,1],[0,1],\"--\")\n",
        "            plt.title(\"ROC curve\")\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate\")\n",
        "            plt.legend(loc=\"lower right\")\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(\"ROC plot failed:\", e)\n",
        "\n",
        "        # Confusion matrix heatmap\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title(\"Confusion matrix\")\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(2)\n",
        "        plt.xticks(tick_marks, [\"fake(0)\",\"real(1)\"])\n",
        "        plt.yticks(tick_marks, [\"fake(0)\",\"real(1)\"])\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return {\"accuracy\": acc, \"auc\": auc_score, \"confusion_matrix\": cm, \"report\": cr, \"y_true\": y_true, \"y_scores\": y_scores}\n",
        "\n",
        "\n",
        "# === 8. Utility to plot training curves ===\n",
        "def plot_history(history):\n",
        "    if history is None:\n",
        "        return\n",
        "    hist = history.history\n",
        "    plt.figure(figsize=(12,4))\n",
        "    # video_pred loss\n",
        "    plt.subplot(1,2,1)\n",
        "    if \"loss\" in hist:\n",
        "        plt.plot(hist[\"loss\"], label=\"train_total_loss\")\n",
        "    if \"val_loss\" in hist:\n",
        "        plt.plot(hist[\"val_loss\"], label=\"val_total_loss\")\n",
        "    if \"video_pred_loss\" in hist:\n",
        "        plt.plot(hist[\"video_pred_loss\"], label=\"train_video_loss\")\n",
        "    if \"val_video_pred_loss\" in hist:\n",
        "        plt.plot(hist[\"val_video_pred_loss\"], label=\"val_video_loss\")\n",
        "    plt.title(\"Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # video_pred acc\n",
        "    plt.subplot(1,2,2)\n",
        "    acc_key = None\n",
        "    val_acc_key = None\n",
        "    # find the metric keys\n",
        "    for k in hist.keys():\n",
        "        if k.endswith(\"video_pred_acc\") and not k.startswith(\"val_\"):\n",
        "            acc_key = k\n",
        "        if k.startswith(\"val_\") and k.endswith(\"video_pred_acc\"):\n",
        "            val_acc_key = k\n",
        "    # fallback\n",
        "    if \"video_pred_acc\" in hist:\n",
        "        acc_key = \"video_pred_acc\"\n",
        "    if \"val_video_pred_acc\" in hist:\n",
        "        val_acc_key = \"val_video_pred_acc\"\n",
        "    if acc_key:\n",
        "        plt.plot(hist[acc_key], label=\"train_video_acc\")\n",
        "    if val_acc_key:\n",
        "        plt.plot(hist[val_acc_key], label=\"val_video_acc\")\n",
        "    # also try straight \"video_pred_acc\" keys if available\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# === 9. Run training + evaluation ===\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change epochs and batch_size here\n",
        "    EPOCHS = 8\n",
        "    BATCH_SIZE = 2\n",
        "\n",
        "    detector, history, (train_p, val_p, train_l, val_l) = train(base_dir=\"/content/deepfake_dataset\", epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Show training curves\n",
        "    plot_history(history)\n",
        "\n",
        "    # Evaluate on full validation set (prints and plots ROC & confusion matrix)\n",
        "    eval_results = evaluate_on_validation(detector, val_p, val_l, plot_figures=True)\n",
        "\n",
        "    # Optionally save the eval results to disk\n",
        "    import json\n",
        "    with open(\"/content/eval_results.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"accuracy\": float(eval_results[\"accuracy\"]),\n",
        "            \"auc\": float(eval_results[\"auc\"]),\n",
        "            \"confusion_matrix\": eval_results[\"confusion_matrix\"].tolist()\n",
        "        }, f)\n",
        "    print(\"Saved evaluation summary to /content/eval_results.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}